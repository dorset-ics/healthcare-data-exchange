{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>The Healthcare Data Exchange is a FHIR based integration and interoperability platform to support a regional healthcare network.</p> <p>The solution integrates with UK national services such as the Personal Demographics Service, and National Data Opt-Out, and provides various options to ingest data from a typical NHS organisation.</p> <p>Data is mapped to the UK Core R4 FHIR profiles, and made available through a standards compliant FHIR REST API.</p> <p>For more information on UK Core, navigate to the following page.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Check out our Getting Started guide to get up and running with the solution for local development. This guide will walk you through the project structure and setup guide for running and developing the application locally.</p> <p>If you are interested in adopting this solution in your organisation, check out the Setup Guide for more information on how to setup the project in your organisation.</p> <p>Also, check out the developer folder for more information on development guidelines and development and testing on the cloud.</p>"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#project-structure","title":"Project Structure","text":"<p>This project is structured into several parts:</p> <ul> <li><code>src</code>: This is where the main application code resides. It's a .NET 8.0 application.</li> <li><code>tests</code>: This is where the tests reside.</li> <li><code>templates</code>: This directory contains the liquid templates which are used during the FHIR mapping and conversion operations.</li> <li><code>docker</code>: This directory contains the Docker Compose file used to run the application locally, along with its dependencies and mounted storage volume(s).</li> <li><code>docs</code>: Contains the project documentation, which can be built into a static site using MkDocs, and supplied configuration and docker-compose file.</li> <li><code>data</code>: contains sample and test data.</li> </ul>"},{"location":"getting-started/#local-development","title":"Local Development","text":"<p>See below for details to get up and running.</p> <p>Further information for developers wanting to work on the project can be found in the developer getting started guide.</p>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker</li> <li>.NET SDK 8</li> <li>IDE of choice</li> </ul>"},{"location":"getting-started/#running-the-api-and-its-dependencies","title":"Running the API and its dependencies","text":"<p>The docker-compose file located under the 'docker' folder will build and run a containerized version of the application, along with its dependencies such as the OSS FHIR server and the SQL Server database for the FHIR server, as well as an Azurite Azure Storage Emulator container for local emulation of Azure Blob Storage. See more information on the Azure Storage Emulator.</p>"},{"location":"getting-started/#prerequisite-set-environment-variables","title":"Prerequisite: Set environment variables","text":"<p>Copy the contents of the <code>docker/.env.template</code> file to a new <code>.env</code> file:</p> <pre><code>cp docker/.env.template docker/.env\n</code></pre> <p>Fill in the missing fields:</p> <ul> <li>AZURE_CLIENT_ID</li> <li>AZURE_CLIENT_SECRET</li> <li>AZURE_TENANT_ID</li> <li>AZURE_STORAGE_CONNECTION_STRING</li> <li>SAPASSWORD - Please make sure to comply with the SQL Server password policy</li> <li>TAG</li> <li>DataHubFhirServer__TemplateImage - Update the value to the image name of the templates image in your Azure Container Registry</li> </ul>"},{"location":"getting-started/#option-1-running-the-application-using-the-all-in-one-script","title":"Option 1: Running the application using the all-in-one script","text":"<p>To run all the required services using a script, run the following command from the root of the repository:</p> <pre><code>bash ./docker/start-clean.sh\n</code></pre> <p>If you'd rather run the services individually or find out more about what the script does under the hood, you can follow the steps below. For more information on the components, see also the developer/development-guide.md.</p>"},{"location":"getting-started/#option-2-running-each-service-individually","title":"Option 2: Running each service individually","text":"<p>For the API to work properly with the templates, you will need to have the templates image in your Azure Container Registry. You can build the image using the Dockerfile in the templates folder and push it to your Azure Container Registry. This is described in Liquid Templates section of the development guide.</p> <p>You will also need to run the data initialization - also described in the Data Initialisation section of the development guide.</p> <p>Once those two services are running, run the following command from the root of the repository:</p> <pre><code>docker-compose -f docker/docker-compose.yml up\n</code></pre> <p>You can also run individual containers by specifying the service name, for example to run the FHIR server (and the SQL Server database it depends on):</p> <pre><code>docker-compose -f docker/docker-compose.yml up fhir-server\n</code></pre> <p>Note: When running on Apple Silicon, you need to set the following environment variable on the <code>fhir-api</code> services: <code>DOTNET_EnableWriteXorExecute=0</code>.</p>"},{"location":"getting-started/#option-3-running-the-application-using-the-net-sdk","title":"Option 3: Running the Application Using the .NET SDK","text":"<p>To run the application using the .NET CLI, run the following commands from the root of the repository:</p> <pre><code>cd src/Api\ndotnet run\n</code></pre> <p>Alternatively, you can run and debug the application from your IDE of choice for faster development iterations. All services defined in the docker-compose have container-to-host port mappings defined, so the application can be run directly and it will still have access to any relevant dependencies.</p>"},{"location":"getting-started/#test-the-setup","title":"Test the Setup","text":"<p>Once the application is running, check if you have the following services running:</p> <p></p> <p>You can test the setup by navigating to the following URLs: http://localhost:8000/Patient?family=Smith&amp;gender=female&amp;birthdate=2010-10-22. You should see a FHIR resource of type Patient with id 9000000009 in JSON format.</p>"},{"location":"getting-started/#running-the-tests","title":"Running the tests","text":"<p>To run the tests, you can use the following command:</p> <pre><code>cd tests\ndotnet test\n</code></pre>"},{"location":"getting-started/#storage-emulation","title":"Storage Emulation","text":"<p>This project uses the Azurite emulator for local Azure Storage development. This is configured a service in the <code>docker-compose.yml</code> file. To connect to the emulator, a \"fake\" connection string is used.</p> <p><code>DefaultEndpointsProtocol=http;AccountName=devstoreaccount1;AccountKey=Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==;BlobEndpoint=http://127.0.0.1:10000/devstoreaccount1;QueueEndpoint=http://127.0.0.1:10001/devstoreaccount1;TableEndpoint=http://127.0.0.1:10002/devstoreaccount1;</code></p> <p>Note: While this connection string includes an <code>AccountKey</code>, it is not deemed a \"secret\". It is an artificial connection string used by the storage emulator only. Details are documented here.</p>"},{"location":"setup-guide/","title":"Setup Guide","text":"<p>This guide will help you set up the repository in your organisation, create a fork, set up deployment pipelines, add Liquid templates for your data-type, and integrate with the Healthcare Data Exchange.</p>"},{"location":"setup-guide/#create-a-fork-of-the-repository","title":"Create a fork of the repository","text":"<p>You will have to create a fork of this repository for your organisation. Below are the steps to do so depending on whether you use Github or Azure DevOps.</p> <p>Please note that the below guidance is for deploying the Healthcare Data Exchange in your organisation. If you are looking to contribute to the original repository, please refer to the contribution guidelines.</p>"},{"location":"setup-guide/#github","title":"Github","text":"<ol> <li>Navigate to the Healthcare Data Exchange repository</li> <li>Click on the <code>Fork</code> button on the top right corner of the page.</li> <li>Select your organisation as the destination for the fork. More detailed instructions at the fork a repo guide.</li> <li>Once the fork is complete, you will be redirected to your forked repository.</li> <li>Clone the repository to your local machine.</li> <li>Please ensure that you create a separate branch for your changes. We recommend using the <code>development</code> branch for your changes (or any branch name of your preference). We use <code>main</code> branch only to sync back the changes from the upstream repository.</li> <li>You can now start making changes to the codebase.</li> <li>To keep your fork up to date with the original repository, you can follow the steps at syncing a fork guide.</li> <li>To contribute back to the original repository, please refer to the contribution guidelines.</li> </ol>"},{"location":"setup-guide/#azure-devops","title":"Azure DevOps","text":"<ol> <li>Navigate to the Healthcare Data Exchange repository</li> <li>Click on the <code>Fork</code> button on the top right corner of the page.</li> <li>Select your organisation as the destination for the fork. More detailed instructions are available at setting a fork in Azure DevOps.</li> <li>Once the fork is complete, you will be redirected to your forked repository in Azure DevOps.</li> <li>Clone the repository to your local machine.</li> <li>Please ensure that you create a separate branch for your changes. We recommend using the <code>development</code> branch for your changes (or any branch name of your preference). We use <code>main</code> only to sync back the changes from the upstream repository.</li> <li>You can now start making changes to the codebase.</li> <li>To keep your fork up to date with the original repository on Github, you can follow the guide for syncing fork in Azure DevOps.</li> <li>To contribute back to the original repository, please refer to the contribution guidelines.</li> </ol>"},{"location":"setup-guide/#diagram-for-forking-and-syncing","title":"Diagram for forking and syncing","text":"<pre><code>graph LR;\n    healthcare-data-exchange-main-branch[healthcare-data-exchange main branch]--&gt;|create a fork|YourFork-main-branch[YourFork main branch];\n    YourFork-main-branch[YourFork main branch]--&gt;YourFork-development-branch[YourFork development branch];\n    healthcare-data-exchange-main-branch--&gt;|sync the repository|YourFork-main-branch;\n    YourFork-feature1[YourFork feature-branch-1]--&gt;YourFork-development-branch[YourFork development branch];\n    YourFork-feature2[YourFork feature-branch-2]--&gt;YourFork-development-branch[YourFork development branch];\n    YourFork-feature3[YourFork feature-branch-3]--&gt;YourFork-development-branch[YourFork development branch];\n</code></pre>"},{"location":"setup-guide/#set-up-keys-and-service-principals-for-running-pipelines","title":"Set up keys and service principals for running pipelines","text":""},{"location":"setup-guide/#github_1","title":"Github","text":"<p>TBD</p>"},{"location":"setup-guide/#azure-devops_1","title":"Azure DevOps","text":"<ol> <li>The first step in Azure DevOps to run pipeline would be to create a <code>Azure Service Principal</code> for your <code>Azure DevOps pipeline</code>. Here are the steps to create the Azure Service Principal.</li> <li>The next step would be to create a <code>Service Connection</code> in Azure DevOps. This <code>Service Connection</code> will be used to authenticate the pipeline execution.</li> <li>The detailed steps are present on the Azure documentation for managing service connections in Azure DevOps.</li> </ol>"},{"location":"setup-guide/#set-up-bvt-build-verification-testing-pipelines","title":"Set up BVT (build verification testing) pipelines","text":""},{"location":"setup-guide/#github-actions-pipeline","title":"Github Actions Pipeline","text":"<ol> <li>As part of the forked repository, you will get the <code>.github/workflows</code> folder in the repository.</li> <li><code>bvt.yaml</code> defines the <code>bvt</code> steps. You can modify the steps as per your requirements.</li> <li>The pipeline uses <code>Azure Container Registry</code> to store the <code>liquid templates</code> against which the <code>template tests</code> are run.</li> <li>You can modify <code>acr_name</code> as per your configurations.</li> </ol>"},{"location":"setup-guide/#azure-devops-pipelines","title":"Azure DevOps Pipelines","text":"<ol> <li>As part of the forked repository, you will get the <code>Pipelines</code> section in Azure DevOps under the <code>.pipelines</code> folder.</li> <li><code>templates/bvt.yaml</code> defines the <code>bvt</code> steps. You can modify the steps as per your requirements.</li> <li>The pipeline uses <code>Azure Container Registry</code> to store the <code>liquid templates</code> against which the <code>template tests</code> are run.</li> <li>You can modify <code>ACR_NAME</code> as per your configurations.</li> </ol>"},{"location":"setup-guide/#set-up-cicd-pipelines","title":"Set up CI/CD pipelines","text":""},{"location":"setup-guide/#github-actions-pipeline_1","title":"Github Actions Pipeline","text":"<ol> <li>Navigate to the <code>.github/workflows</code> folder in the repository.</li> <li>Refer to the existing workflows to create a new workflow for your organisation.</li> </ol>"},{"location":"setup-guide/#azure-devops-pipeline","title":"Azure DevOps Pipeline","text":"<ol> <li>Navigate to the <code>Pipelines</code> section in Azure DevOps.</li> <li>Create a new pipeline or reuse existing one, and select the repository you forked. Refer to Azure DevOps Pipelines for more information.</li> <li>Save and run the pipeline.</li> </ol>"},{"location":"setup-guide/#integrate-with-data-source","title":"Integrate with Data Source","text":"<p>You will have to integrate with the source of the data you want to ingest into the Healthcare Data Exchange. A detailed guide is available at source integration guide.</p>"},{"location":"setup-guide/#add-liquid-templates-for-your-data-type","title":"Add Liquid templates for your data-type","text":"<p>Healthcare Data Exchange repository uses Liquid templates to transform data from the source system to the FHIR format. You will need to create a Liquid template for your data-type. Please refer to the existing templates in the <code>templates</code> folder for existing liquid templates.</p>"},{"location":"setup-guide/#set-up-the-integration-with-healthcare-data-exchange","title":"Set up the Integration with Healthcare Data Exchange","text":"<p>Once your repository is setup within your organisation, you can start integrating with the Healthcare Data Exchange. Here is the detailed integration guide for integrating with the Healthcare Data Exchange.</p>"},{"location":"data-mapping/overview/","title":"Overview","text":""},{"location":"data-mapping/snomed-code-system/","title":"SNOMED Code System","text":"<p>The data ingestion pipeline implements an <code>IFhirResourceEnhancer</code> to map SNOMED codes to their corresponding concept descriptions. This is invoked after data conversion and before the <code>Bundle</code> is persisted.</p> <p>The resource enhancer searches the FHIR bundle for any <code>CodeableConcept</code> elements with the SNOMED system URL. It then queries the SNOMED code system for the respective concept description.</p> <p>This implementation does not currently support terminology queries against a FHIR terminology server. At present, the query is made against a JSON file containing the dictionary of descriptions. This file is located in \"src/Infrastructure/Terminology/SnomedCodes.json\".</p>"},{"location":"data-mapping/snomed-code-system/#using-the-snomed-code-system","title":"Using the SNOMED Code System","text":""},{"location":"data-mapping/snomed-code-system/#populating-the-snomed-code-system","title":"Populating the SNOMED Code System","text":"<p>You must first populate the JSON file (<code>SnomedCodes.json</code>) with the SNOMED codes you wish to map.</p> <p>The file is loaded into memory and used to map SNOMED codes to their descriptions. The file should be structured as follows:</p> <pre><code>{\n  \"123456789\": \"Concept description\",\n  \"987654321\": \"Another concept description\"\n}\n</code></pre>"},{"location":"data-mapping/snomed-code-system/#output-a-codeableconcept-with-snomed-system-url","title":"Output a <code>CodeableConcept</code> with SNOMED System URL","text":"<p>To make use of this enrichment process, simply include a <code>CodeableConcept</code> element in your FHIR resource with the SNOMED system URL. The resource enhancer will automatically lookup the code in the JSON file, and populate its description.</p> <p>For example:</p> <pre><code>{\n  \"resourceType\": \"Procedure\",\n  \"code\": {\n    \"coding\": [\n      {\n        \"system\": \"http://snomed.info/sct\",\n        \"code\": \"123456789\"\n      }\n    ]\n  }\n}\n</code></pre> <p>Will be transformed to:</p> <pre><code>{\n  \"resourceType\": \"Procedure\",\n  \"code\": {\n    \"coding\": [\n      {\n        \"system\": \"http://snomed.info/sct\",\n        \"code\": \"123456789\",\n        \"display\": \"Concept description\"\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"data-mapping/testing-and-validation/","title":"Testing and Validation","text":"<p>This document provides a comprehensive guide on how to run and manage the tests for the liquid templates in this project.</p>"},{"location":"data-mapping/testing-and-validation/#overview","title":"Overview","text":"<p>The test suite is designed to validate the functionality of the liquid templates. It does this by comparing the output of a template for a given input, with an expected output. If an expected output file does not exist, the test will create it. If it does exist, the test will compare the converted version with the expected output. When a template is modified, the author should commit the relevant outputs, which will then be presented in the PR for review.</p>"},{"location":"data-mapping/testing-and-validation/#directory-structure","title":"Directory Structure","text":"<p>The directory structure for the tests is designed to mirror the structure of the <code>templates</code> directory. It is as follows:</p> <ul> <li><code>templates/Organisations/&lt;organisation-code&gt;/&lt;source-domain&gt;/&lt;data-type&gt;/&lt;resource type&gt;/</code>: This directory contains the   liquid templates that are being tested.</li> <li><code>tests/Templates.Tests/input/Organisations/&lt;organisation-code&gt;/&lt;source-domain&gt;/&lt;data-type&gt;/&lt;resource type&gt;/</code>: This directory   contains the sample input files for the tests. The file name should be a combination of the resource type and the   number of the example. For example, <code>x26/pds-mesh/json/patient/Patient-01.json</code>.</li> <li><code>tests/Templates.Tests/output/Organisations/&lt;organisation-code&gt;/&lt;source-domain&gt;/&lt;data-type&gt;/&lt;resource type&gt;/</code>: This directory   will contain the output of the tests.</li> </ul>"},{"location":"data-mapping/testing-and-validation/#test-flow","title":"Test flow","text":"<p>The <code>Template.Tests</code> project includes a test that retrieves a list of all input files and creates a test case for each one. Each test case is using the <code>Integration</code> version of the API and uses its <code>DataHubFhirClient</code> to call the <code>$convert-data</code> endpoint of the FHIR service. Following this, a template validation is performed against the <code>$validate</code> endpoint. Only after these steps are completed is the result of <code>$convert-data</code> compared with the expected output.</p>"},{"location":"data-mapping/testing-and-validation/#running-the-tests","title":"Running the Tests","text":""},{"location":"data-mapping/testing-and-validation/#pre-requisites","title":"Pre-requisites","text":""},{"location":"data-mapping/testing-and-validation/#set-environment-variables","title":"Set Environment Variables","text":"<p>Before running the tests, make sure that you have a <code>.env</code> file similar to the <code>.env.template</code> file in the <code>/docker</code> directory. you must fill the following variables:</p> <ul> <li><code>AZURE_CLIENT_SECRET</code> : the secret of the service principal that has access to the Azure ACR repository.</li> <li><code>TAG</code>: the tag of the Azure Container Registry image that the templates will be pushed to.</li> </ul> <p>for example:</p> <pre><code>...\n# Templates tag to be pushed to ACR\nTAG=JoeyTemplates001\n</code></pre> <p>For further details about Docker environment files see this link.</p>"},{"location":"data-mapping/testing-and-validation/#initialise-environment","title":"Initialise Environment","text":"<p>Before running the tests, be sure to build and run the required docker containers using the following command:</p> <pre><code>docker-compose -f ./docker/docker-compose.yml up templates-pusher data-init -d --build --force-recreate\n</code></pre> <p>NOTE: the <code>-d</code> (detached) flag is used to run the services in the background, if you want to see the logs of the services in the terminal, you can remove the <code>-d</code> flag.</p> <p>the docker compose file will start a local fhir server with its required sql db, and the <code>templates-pusher</code> service that will push the templates to the azure container registry, and the <code>data-init</code> service that will initialize the fhir server with the required data and profiles.</p> <p>The <code>templates-pusher</code> container is expected to run and then exit. To confirm if the templates were pushed, view the <code>templates-pusher</code> container logs. You can also verify that the specified tag exists in Azure ACR repository for <code>dev</code>, by checking the status in the Azure portal:</p> <ol> <li>Open the Azure portal</li> <li>Navigate to the ACR</li> <li>Open the Repositories screen</li> <li>Open the <code>dev</code> repository</li> <li>Check your tag exists and that Last Modified date is as expected</li> </ol>"},{"location":"data-mapping/testing-and-validation/#troubleshooting","title":"Troubleshooting","text":"<p>If there are any issues, such as \"az login\" needed, or an \"invalid client secret is provided\", run <code>docker-compose build --no-cache</code> followed by <code>docker-compose up --force-recreate templates-pusher -d</code>.</p>"},{"location":"data-mapping/testing-and-validation/#run-the-tests","title":"Run the Tests","text":"<p>Once the docker containers are running and healthy, you can run the tests using your IDE's test runner.</p> <p>Your IDE's test runner should discover the tests in test runtime since the test cases are dynamically built based on the input folder.</p> <p>Alternatively, you can run the tests using the command line interface (CLI), which is also the method used in our Continuous Integration (CI) process:</p> <pre><code>dotnet test tests/Templates.Tests/Templates.Tests.csproj -e DataHubFhirServer:TemplateImage=acrdexoss.azurecr.io/dev:MYTAG\n</code></pre> <p>NOTE: The <code>-e</code> flag is used to pass environment variables to the test runner. The <code>DataHubFhirServer:TemplateImage</code> variable is used to override the image tag that the tests will use to run the templates. The <code>MYTAG</code> should be replaced with the tag of the image that the templates were pushed to in the previous step.</p>"},{"location":"data-mapping/testing-and-validation/#debug-a-single-test","title":"Debug a Single Test","text":"<p>Since the test inputs are dynamically built based on the input directory, all tests will have to run. However, you can debug a single test input by applying a conditional breakpoint in the test code. For example, you could use <code>relativePath.Contains(\"A03-01\")</code> to isolate a specific test input.</p>"},{"location":"data-mapping/testing-and-validation/#assumptions","title":"Assumptions","text":"<p>The tests assume that the Docker services are running and that the <code>templates-pusher</code> and <code>data-init</code> have been started manually. It also assumes that the <code>.env</code> file has been created and filled with the required variables, as described in the \"Pre-requisites\" section.</p>"},{"location":"data-mapping/mappings/ndop-fhir/","title":"National Data Opt-Out FHIR Mapping","text":""},{"location":"data-mapping/mappings/ndop-fhir/#overview","title":"Overview","text":"<p>The National Data Opt-out Programme (NDOP) allows patients to choose if they wish to only allow their personally identifiable data used for their care and treatment, or if they wish to allow their data to be shared with other healthcare organisations for planning and research purposes.</p> <p>The NDOP FHIR API can be used to retrieve the Consent preferences of a patient by querying their NHS Number.</p>"},{"location":"data-mapping/mappings/ndop-fhir/#mapping","title":"Mapping","text":"<p>The Consent template contains the Liquid template for converting an NDOP MESH Consent resource into UK Core Consent.</p> <p>The FHIR-Converter provides a template for converting from Stu3 to R4. Those can be found in the FHIR-Converter repository and includes:</p> <ul> <li><code>Consent.liquid</code></li> <li><code>Consent/_Base.liquid</code></li> <li><code>Consent/_Provision.liquid</code></li> <li><code>DataType/CodingToCodeableConcept.liquid</code></li> <li><code>Resource.liquid</code></li> </ul> <p>These templates have been modified to include any requirements for UK Core Consent which extends R4.</p> <p>In summary, the main modifications are:</p> <ul> <li>mergeDiff: The original template uses a custom <code>mergeDiff</code> tag which allows the author to specify any mappings, whilst also merging any other fields which are unchanged. As we do not use <code>mergeDiff</code> in this repository, the merge is done manually, by including all the fields explicitly.</li> <li>Resource: The original template references a <code>Resource</code> template which itself references dozens of other templates. To avoid including many unused files, the <code>Resource</code> file has been edited to include only the parts relating to <code>Consent</code>.</li> <li>UK Core requirements: There are some required fields in UK Core which are not in NDOP Mesh. These have been added, but temporarily filled with placeholder values.</li> <li>References: The NDOP Mesh input includes literal references - these are translated to logical references so that these values can be later updated once the new literal references are known.</li> </ul> Source Destination Comment <code>resourceType</code> <code>resourceType</code> No change from FHIR-Converter template <code>id</code> <code>id</code> No change from FHIR-Converter template <code>status</code> <code>status</code> No change from FHIR-Converter template <code>scope</code> Added - required in UK Core Consent but not MESH <code>category</code> Added - required in UK Core Consent but not MESH <code>patient</code> <code>patient</code> No change from FHIR-Converter template <code>dateTime</code> <code>dateTime</code> No change from FHIR-Converter template <code>contained</code> <code>contained</code> No change from FHIR-Converter template <code>identifier</code> <code>identifier</code> No change from FHIR-Converter template <code>organization</code> <code>organization</code> Modified literal reference to logical reference using new <code>Consent/_Organization.liquid</code> <code>policy</code> <code>policy</code> No change from FHIR-Converter template <code>policyRule</code> <code>policyRule</code> No change from FHIR-Converter template <code>sourceIdentifier</code> <code>sourceIdentifier</code> No change from FHIR-Converter template <code>except</code> <code>provision.except</code> No change from FHIR-Converter template <code>data</code> <code>provision.data</code> No change from FHIR-Converter template <code>dataPeriod</code> <code>provision.dataPeriod</code> No change from FHIR-Converter template <code>purpose</code> <code>provision.purpose</code> No change from FHIR-Converter template <code>securityLabel</code> <code>provision.securityLabel</code> No change from FHIR-Converter template <code>action</code> <code>provision.action</code> No change from FHIR-Converter template <code>actor</code> <code>provision.actor</code> No change from FHIR-Converter template <code>consentingParty</code> <code>performer</code> FHIR-Converter template renamed this to <code>performer</code>. Additionally, modified literal reference to logical reference using new <code>Consent/_ConsentingParty.liquid</code> <code>period</code> <code>provision.period</code> No change from FHIR-Converter template"},{"location":"data-mapping/mappings/ndop-fhir/#test-cases","title":"Test Cases","text":"<p>The Consent liquid template has two associated test input files. These are:</p> <ul> <li><code>SampleConsent_Consent.json</code>: This is based on this example https://developer.nhs.uk/apis/optout/Examples/NDOP-Bundle-Searchset-Example-2.json. It is used to show converting a <code>Consent</code> resource generated by NDOP MESH into a UK Core Consent resource.</li> <li><code>SampleMultiple_Consent.json</code>: This is a modification of the example above. It adds multiple references to the <code>organization</code> array and the <code>consentingParty</code> array. This is to test the conversion of multiple literal references into multiple logical references, rather than just a single one.</li> </ul>"},{"location":"data-mapping/mappings/ndop-mesh/","title":"National Data Opt-Out MESH Mapping","text":""},{"location":"data-mapping/mappings/pds-mesh/","title":"PDS MESH Mapping","text":"<p>This is a table specifying PDS mesh response attributes and the FHIR resource attributes they map to.</p> Source Destination Comment UniqueReference id, Bundle.fullUrl NhsNumber identifier.value FamilyName name.family GivenName name.given OtherGivenName name.given Gender gender Gender (sex) of the person, values: 0 = Not Known, 1 = Male, 2 = Female, 9 = Not Specified DateOfBirth birthDate In one of the following formats: full date and time (YYYYMMDDHHMM), full date (YYYYMMDD), year &amp; month (YYYYMM), year only (YYYY) AddressLine1 address.line AddressLine2 address.line AddressLine3 address.line AddressLine4 address.line AddressLine5 address.line Postcode address.postcode TelephoneNumber telecom MobileNumber telecom EmailAddress telecom GpPracticeCode generalPractitioner.identifier"},{"location":"design/architecture/","title":"Architecture","text":"<p>This document is an evolving description and diagram of the Healthcare Data Exchange Architecture.</p> <p></p>"},{"location":"design/architecture/#data-flow","title":"Data flow","text":"<p>Healthcare Data Exchange integrates with multiple consumers, who query data from it. It also integrates with providers, who update the data within it. The data from the providers sources can be either pushed to the platform, or pulled from the sources, it can be in multiple formats. The data then is normalised, going through conversions and mappings, to be transformed into a unified FHIR format, that is persisted in the platform into a FHIR service. This format is the format in which data is queried from the platform by its consumers, some client applications like an ED management system. To orchestrate pushing, pulling, and transforming the incoming data, Healthcare Data Exchange holds a component that acts as a facade for the FHIR service.</p> <ol> <li>Data is ingested into the platform, by the facade, by pulling from the sources, e.g. reading a message from a MESH mailbox, or with RESTful API calls.</li> <li>Data is pushed to the platform, to the facade, by accepting REST HTTP requests, in FHIR or HL7 formats, or streaming CSV / files in the request body. In case a source is not able to push files in HTTP requests, an 'agent' application might be deployed on the client side, that will push the data to the platform.</li> <li>Ingesting patients into the system might also be triggered by GET requests for patient search, in turn triggering update / creation of new patients based on PDS (the main NHS patient index).</li> <li>The ingested data flows in the facade, between several components.<ul> <li>A conversion component converts the data into FHIR R4, a mapping component maps the FHIR R4 into a unified FHIR that aligns all attributes, and maps them to FHIR UK Core profile.</li> <li>Conversion and mapping would use (liquid) templates which will be used by the Azure Health FHIR services (REST $convert-data API call).</li> <li>The template are pushed during application deployment, as OCI artifacts to Azure Container Registry.</li> <li>Those templates are meant to be easy to write for a Healthcare Data Exchange integrator.</li> <li>In case needed, the data in its raw form will be written, before conversion, into a blob storage, for secondary use purposes.</li> <li>A final component might hold business logic that is required post-conversion, and before persisting the records, like upserting, updating references between existing FHIR records, or any other post-processing operations.</li> </ul> </li> <li>Data is persisted into a managed 'Azure Health Data Services' FHIR service.</li> <li>Queriers get data from the platform by GET requests, routed into the FHIR service by an Azure Api Management service.</li> <li>At known intervals, data is propagated into a Data lake for secondary use, taking into consideration the patients data opt-out preferences. It can later be queried in FHIR/OMOP formats for analytics / data science purposes.</li> </ol> <p></p>"},{"location":"design/architecture/#azure-and-networking","title":"Azure and networking","text":"<p>Healthcare Data Exchange is meant to be deployed as a solution over the Azure cloud.</p> <ol> <li>Application gateway will receive all incoming REST HTTPS requests acting as the system entrypoint. It will also act as a load balancer, and have the Web Application Firewall (WAF) enabled. It will be deployed under a dedicated subnet, and will be the only component exposed to the internet.</li> <li>API Management will enforce Authentication and Authorization, integrating with Microsoft Entra ID to validate the JWT tokens based on Application Roles. It will route requests based on their verb and resource to either the Facade or directly to the FHIR service. It will be deployed under a dedicated subnet, and linked as a single 'backend pool' to the Application Gateway.</li> <li>The FHIR Facade will be deployed in an Azure Web App Service, under an App Service Plan, holding all components described in the Data flow section, deployed under a subnet with virtual network integration.</li> <li>The FHIR service will be provisioned as the managed FHIR service that is part of the Azure Health Data Services.</li> <li>Azure storage will hold the blob storages that will be used for intermediate persistence.</li> <li>Application insights and log analytics managed services will provide insights and observations on the whole platform.</li> <li>Azure container registry (ACR) will hold artifacts like docker images, from which the services will be deployed, and the liquid templates for conversion and mappings (to be consumed by the FHIR service).</li> <li>Azure Key vault will hold secrets and certificates and configurations, that will be used by the platform.</li> <li>Azure data lake will hold data for secondary use, and later, might be migrated into an Azure Fabric managed offering.</li> <li>All subnets will be part of a single core VNET.</li> <li>All SaaS offerings / resources that are not in the core VNET, will be linked to the VNET with private links when possible.</li> </ol> <p></p>"},{"location":"design/security-plan/","title":"Security Plan","text":""},{"location":"design/facade/api/","title":"Api","text":""},{"location":"design/facade/api/#ingestion-endpoint","title":"Ingestion Endpoint","text":"<p>The ingestion endpoint is a crucial part of our API, serving as the primary interaction point for the integration engines. It is responsible for receiving and processing HL7v2 messages from external sources, and is ready to be expanded to more types of messages.</p>"},{"location":"design/facade/api/#endpoint-details","title":"Endpoint Details","text":"<p>URL: <code>/$ingest</code></p> <p>Method: <code>POST</code></p> <p>Content-Type: <code>text/plain</code></p>"},{"location":"design/facade/api/#request","title":"Request","text":"<p>The ingestion endpoint expects an HL7v2 message in the body of the POST request. The <code>source-domain</code>, <code>organisation-code</code>, and <code>data-type</code> are expected to be provided in the headers of the request. Here is an example of a request:</p> <pre><code>POST /$ingest HTTP/1.1\norganisation-code: uhd\nsource-domain: agyleed\ndata-type: hl7v2\nContent-Type: text/plain\n\nMSH|^~\\&amp;|AGYLEED|R0D02|INTEGRATION-ENGINE|RDZ|20231127125907||ADT^A01|667151|P|2.4|||AL|NE\nPID|1|0123456|0123456^^^NHS^HospitalNumber~9999999999^^^NHS^NhsNumber||SURNAME^NAME||19570830|M|||STREET^STREET2^POOLE^DORSET^BH15 3RS^^H||01234000000^PRN^PH~07000000000^PRN^CP|01234000000||||||||A\nPV1|1|E||F||||||||||507291000000100|||||ED-AG-23-000000|||||||||||||||||||||||^^^R0D02||20231127125458\nPV2|||Injury of shoulder / arm / elbow / wrist / hand|||||||||||||||||||||||||||||||||||1048071000000103\n</code></pre> <ul> <li><code>organisation-code</code>: The code representing the organisation sending the data.</li> <li><code>source-domain</code>: The domain system from which the data is being sent.</li> <li><code>data-type</code>: would be set by the integration engine based on what type of messages are we receiving (hl7v2/csv/etc)</li> <li><code>Content-Type</code>: The type of the content being sent. In this case, it is <code>text/plain</code> as we are sending an HL7v2 message.</li> </ul>"},{"location":"design/facade/api/#response","title":"Response","text":"<p>The ingestion endpoint responds with a <code>plain/text</code> result containing the status of the ingestion process. Here is an example of a response payload:</p> <pre><code>MSH|^~\\&amp;|DEX|QVV|domain|org|20240131084246||ACK|3d749df9-caa5-42e2-91bb-4f9a24a04c9d|P|2.4\nMSA|AA|some-string|Successfully processed\n</code></pre>"},{"location":"design/facade/api/#status-codes","title":"Status Codes","text":"<ul> <li><code>200 OK</code>: The request was successful, and the data was ingested correctly.</li> <li><code>400 Bad Request</code>: The request was malformed or missing required data, or the data type is not supported</li> <li><code>500 Internal Server Error</code>: An error occurred on the server while processing the request, template not found, etc.</li> </ul>"},{"location":"design/facade/api/#template-name-building","title":"Template Name Building","text":"<p>The template name is built using the organisation-code, source-domain and data-type from the headers and resource type from the request payload.</p> <p>The format is <code>{organisation-code}_{source-domain}_{data-type}_{resourceType}</code>.</p> <p>For example, if the source-domain is <code>agyleed</code>, organisation-code is <code>uhd</code>, data-type is <code>hl7v2</code> and resource type is <code>adt_a01</code>, the template name will be <code>agyleed_uhd_hl7v2_adta01</code>.</p>"},{"location":"design/facade/api/#global-exception-handler","title":"Global Exception Handler","text":""},{"location":"design/facade/api/#overview","title":"Overview","text":"<p>The Global Exception Handler is a centralized mechanism for handling exceptions in our application. It provides a consistent response structure for all unhandled exceptions, making it easier for clients to handle errors.</p>"},{"location":"design/facade/api/#implementation","title":"Implementation","text":"<p>The Global Exception Handler is implemented as a class that implements the <code>IExceptionHandler</code> interface. It uses the <code>ILogger</code> service to log exceptions and the <code>IHostEnvironment</code> service to determine the current environment.</p> <p>The implementation of the <code>GlobalExceptionHandler</code> class is under the file <code>src/Api/Exceptions/GlobalExceptionHandler.cs</code>.</p> <p>In that file we have a mapping function that maps an exception to a status code and title, and a <code>HandleAsync</code> method that handles the exception and returns a <code>ProblemDetails</code> object.</p> <p>currently the <code>MapException</code> function only maps the <code>Exception</code> class to a 500 status code and \"Internal Server Error\" title. but we can add more mapping for other exceptions like TimeoutException, ArgumentException, etc.</p> <pre><code>\n    private static (int statusCode, string title) MapException(Exception exception)\n        =&gt; exception switch\n        {\n            _ =&gt; (StatusCodes.Status500InternalServerError, \"Internal Server Error\")\n        };\n}\n</code></pre>"},{"location":"design/facade/api/#enabling-the-global-exception-handler","title":"Enabling the Global Exception Handler","text":"<p>To enable the Global Exception Handler, you need to register it in the <code>Startup</code> class:</p> <pre><code>public static IServiceCollection AddApi(this IServiceCollection services)\n    =&gt; services.AddRequestTimeouts()\n        .AddGlobalExceptionHandling() // here\n        .AddEndpointsAndSwagger()\n        .AddFhirJsonSerializer()\n        .AddPatientSearchStrategies()\n        .AddFluentValidators();\n\nprivate static IServiceCollection AddGlobalExceptionHandling(this IServiceCollection services) =&gt;\n    services.AddExceptionHandler&lt;GlobalExceptionHandler&gt;()\n        .AddProblemDetails();\n</code></pre> <p>In the above code, the <code>AddGlobalExceptionHandling</code> extension method is used to register the <code>GlobalExceptionHandler</code> with the dependency injection container. This method is part of the <code>DependencyInjection</code> class in the <code>src/Api/DependencyInjection.cs</code> file.</p>"},{"location":"design/facade/api/#error-response","title":"Error Response","text":"<p>The Global Exception Handler returns a JSON response with a <code>ProblemDetails</code> object. This object includes the status code, title, trace ID, and instance of the error. In a development and staging environment, it also includes the detail of the error.</p> <p>Here's an example of what the error response might look like in a development/staging environment:</p> <pre><code>{\n    \"status\": 500,\n    \"title\": \"Internal Server Error\",\n    \"traceId\": \"0HLN8KNV7BB6S:00000001\",\n    \"instance\": \"GET /Patient?nhs-number=1234567890\",\n    \"detail\": \"An unhandled exception occurred.\"\n}\n</code></pre> <p>In a production environment, the <code>detail</code> field is omitted:</p> <pre><code>{\n    \"status\": 500,\n    \"title\": \"Internal Server Error\",\n    \"traceId\": \"0HLN8KNV7BB6S:00000001\",\n    \"instance\": \"GET /api/values\"\n}\n</code></pre>"},{"location":"design/facade/infrastructure/","title":"Infrastructure","text":"<p>The <code>Infrastructure</code> project is a .NET library that provides an easy way to interact with the Personal Demographics Service (PDS) and the DataHub FHIR server. It includes service configurations and clients for making requests to the PDS and the FHIR server, as well as a client for sending messages to the NHS Mailbox (Mesh).</p>"},{"location":"design/facade/infrastructure/#structure","title":"Structure","text":"<p>The project is divided into several folders:</p> <ol> <li>Server libraries -e.g. PDS and DataHub, each with its own configuration, health check, client and tests. The goal behind them is having the option in the future to split them up into their own independent projects.</li> <li>Common - used across the different server libraries.</li> </ol>"},{"location":"design/facade/infrastructure/#prerequisites","title":"Prerequisites","text":"<ul> <li>.NET 5.0 or later</li> <li>A PDS account with the necessary permissions</li> <li>A Mesh mailbox set up</li> <li>An Azure Fhir server set up or running the FHIR OSS server locally</li> </ul>"},{"location":"design/facade/infrastructure/#configuration","title":"Configuration","text":"<p>The library uses the <code>PDS</code>,<code>Mesh</code> and <code>DataHubFhirServer</code> sections in the <code>appsettings.json</code> file for configuration. Here is an example:</p> <pre><code>{\n    \"PDS\": {\n        \"BaseUrl\": \"&lt;PROD_PDS_SERVER_URL_HERE&gt;\",\n        \"Authentication\": {\n            \"IsEnabled\": false,\n            \"TokenUrl\": \"&lt;TOKEN_URL_HERE&gt;\",\n            \"ClientId\": \"&lt;CLIENT_ID_HERE&gt;\",\n            \"Kid\": \"&lt;KID_HERE&gt;\"\n        },\n        \"Mesh\": {\n            \"SendSchedule\": \"0 0 * * * ?\",\n            \"RetrieveSchedule\": \"0 0 0 * * ?\",\n            \"MailboxId\": \"&lt;MAILBOX_ID&gt;\",\n            \"Key\": \"&lt;KEY&gt;\",\n            \"WorkflowId\": \"SPINE_PDS_MESH_V1\"\n        }\n    },\n    \"Mesh\": {\n        \"Url\": \"http://host.docker.internal:8700\",\n        \"MaxChunkSizeInMegabytes\": 100\n    },\n    \"DataHubFhirServer\": {\n        \"BaseUrl\": \"&lt;PROD_FHIR_SERVER_URL_HERE&gt;\"\n    }\n}\n</code></pre>"},{"location":"design/facade/infrastructure/#mesh-client-configuration","title":"Mesh Client Configuration","text":"<p>The <code>MeshClient</code> uses the <code>Mesh</code> section in the <code>appsettings.json</code> as a base for configuration. and every use case implement it's own concrete mesh client, so for example the <code>PdsMeshClient</code> uses the <code>PDS.Mesh</code> section in the <code>appsettings.json</code> file for configuration. Here is an example:</p> <pre><code>{\n    \"Pds\": {\n        \"Mesh\": {\n            \"SendSchedule\": \"0 0 * * * ?\",\n            \"RetrieveSchedule\": \"0 0 0 * * ?\",\n            \"MailboxId\": \"&lt;MAILBOX_ID&gt;\",\n            \"Key\": \"TestKey\",\n            \"WorkflowId\": \"SPINE_PDS_MESH_V1\"\n        }\n    }\n}\n</code></pre>"},{"location":"design/facade/infrastructure/#usage","title":"Usage","text":"<p>To use this library in your project, you need to add it to your service collection in the <code>Program.cs</code> file:</p> <pre><code>builder.Services.AddInfrastructure(Configuration);\n</code></pre> <p>This will register the <code>IPdsServiceClient</code>, <code>IMeshClient</code> and <code>IDataHubFhirClient</code> services, which you can then inject into your classes:</p> <pre><code>public class MyClass\n{\n    private readonly IPdsClient _pdsClient;\n    private readonly IMeshClient _meshClient;\n    private readonly IDataHubFhirClient _DataHubFhirClient;\n    public MyClass(IPdsClient pdsClient, IMeshClient meshClient, IDataHubFhirClient DataHubFhirClient)\n    {\n        _pdsClient = pdsClient;\n        _meshClient = meshClient;\n        _DataHubFhirClient = DataHubFhirClient;\n    }\n}\n\n// or using default constructors\npublic class MyClass(IPdsClient pdsClient, IMeshClient meshClient, IDataHubFhirClient DataHubFhirClient)\n</code></pre> <p>note that <code>AddPdsInfrastructure</code> and <code>AddDataHubFhirInfrastructure</code> are private methods used internally by <code>AddInfrastructure</code> to set up the necessary services.</p> <pre><code>var sendMessageAsync = await meshClient.Mailbox.SendMessageAsync(\n            mexTo: mailBoxId,\n            mexWorkflowId: Guid.NewGuid().ToString(),\n            content: \"Hello World!\",\n            mexSubject: \"Subject\",\n            mexLocalId: \"1\",\n            mexFileName: \"test.txt\",\n            contentType: MediaTypeNames.Text.Plain);\n</code></pre>"},{"location":"design/facade/infrastructure/#updates-from-pds","title":"Updates From PDS","text":"<p>Updates from PDS are received via a Mesh Mailbox update, with a process titled PDS Trace. The process starts by requesting such trace data for every patient in the FHIR store, effectively exporting the entire FHIR patients store to a CSV file to be sent to the Mesh Mailbox. The integration with PDS is described here. The API will schedule a PDS Trace process to run at configurable intervals, sending a full Patient record dataset to the Mesh Mailbox. The configuration for this process looks like this:</p> <pre><code>{\n    \"PDS\": {\n        \"Mesh\": {\n            \"SendSchedule\": \"0 0 0 * * ?\",\n            \"MailboxId\": \"&lt;MAILBOX_ID&gt;\"\n        }\n    }\n}\n</code></pre> <p>The process is scheduled using the Quartz.NET library that triggers the main method of ScheduledDataHubToPdsMesh class instance, and the schedule is defined using a cron expression.</p>"},{"location":"design/facade/infrastructure/#health-checks","title":"Health Checks","text":"<p>This application includes health checks for any external service to the Api, such as: DataHub FHIR service, PDS FHIR service, Mesh mailbox. Health checks are used to probe the state of the application and its dependencies to determine its health.</p>"},{"location":"design/facade/infrastructure/#overview","title":"Overview","text":"<p>The health checks in this application are implemented using the <code>IHealthCheck</code> interface provided by the <code>Microsoft.Extensions.Diagnostics.HealthChecks</code> package. This interface requires implementing a <code>CheckHealthAsync</code> method that returns a <code>HealthCheckResult</code> as seen in the official docs</p> <p>The <code>DataHubFhirHealthCheck</code> and <code>PdsFhirHealthCheck</code> and <code>MeshMailboxHealthCheck</code>  classes are specific health checks for the DataHub FHIR service and the PDS FHIR service respectively. They inherit from the <code>BaseFhirHealthCheck</code> abstract class, which provides a common implementation for calling a service health check endpoint, then gets validated by the <code>HealthCheckResult</code> class.</p>"},{"location":"design/facade/infrastructure/#how-health-checks-work","title":"How Health Checks Work","text":"<p>In the <code>CheckHealthAsync</code> method, an HTTP GET request is sent to the health check endpoint of the respective FHIR service. This is done using an <code>HttpClient</code> that is retrieved from an <code>IHttpClientFactory</code>. The <code>HttpClient</code> is configured with the name of the client (<code>ClientName</code>) and the health check endpoint (<code>HealthCheckEndpoint</code>), which are both provided by the derived classes.</p> <p>If the FHIR service responds with a successful HTTP status code (i.e., in the range 200-299), the health check is considered to have passed and <code>HealthCheckResult.Healthy()</code> is returned. If the service responds with a non-successful status code, or if an exception occurs during the request, the health check is considered to have failed and <code>HealthCheckResult.Unhealthy()</code> is returned.</p>"},{"location":"design/facade/infrastructure/#using-the-health-checks","title":"Using the Health Checks","text":"<p>The health checks are registered in the dependency injection container with the <code>AddCheck</code> method over the <code>services.AddHealthChecks()</code> extension. The health checks are then run by the health check middleware when a GET request is sent to the <code>/_health</code> endpoint of the application.</p> <p>The response from the <code>/_health</code> endpoint is a JSON written by <code>AspNetCore.HealthChecks.UI.Client.UIResponseWriter</code> class, that includes the overall health status of the application, the total duration of all health checks, and an array of entries for each health check. Each entry includes the name of the health check, the duration of the health check, the health status, and any tags associated with the health check.</p> <p>Here is an example of what the response might look like:</p> <pre><code>{\n    \"status\": \"Healthy\",\n    \"totalDuration\": \"00:00:00.1612859\",\n    \"entries\": {\n        \"DataHub FHIR Health Check\": {\n            \"data\": {},\n            \"duration\": \"00:00:00.0147493\",\n            \"status\": \"Healthy\",\n            \"tags\": [\n                \"FHIR\",\n                \"DataHub\",\n                \"Api\"\n            ]\n        },\n        \"PDS FHIR Health Check\": {\n            \"data\": {},\n            \"duration\": \"00:00:00.1610403\",\n            \"status\": \"Healthy\",\n            \"tags\": [\n                \"FHIR\",\n                \"PDS\",\n                \"Api\"\n            ]\n        },\n        \"Mesh Mailbox Health Check\": {\n            \"data\": {},\n            \"duration\": \"00:00:00.0158644\",\n            \"status\": \"Healthy\",\n            \"tags\": [\n                \"Mesh\",\n                \"Background\"\n            ]\n        }\n    }\n}\n</code></pre>"},{"location":"design/facade/overview/","title":"Overview","text":"<p>Our facade application is structured according to the principles of Clean Architecture. Here's a brief overview of the main components:</p> <ul> <li><code>src/Api</code>: This is the entry point of our application. It contains the web API controllers (carter modules) and   startup configuration.</li> <li><code>src/Core</code>: This layer contains the business logic and entities of our application.</li> <li><code>src/Infrastructure</code>: This layer contains classes for accessing external resources such as databases (DataHub/PDS),   file systems and external services.</li> <li><code>tests</code>: This directory contains all the test projects for our application.</li> </ul> <p>By adhering to the principles of Clean Architecture, we aim to create a system that is independent of UIs, databases, frameworks, and external agencies. This makes our system testable, independent of the UI, independent of the database, independent of any external agency, and organized around use cases and features.</p>"},{"location":"design/facade/overview/#clean-architecture","title":"Clean Architecture","text":"<p>Clean Architecture divides the system into layers, each with its own responsibility. The dependencies between these layers follow the Dependency Rule, which states that dependencies should point inwards towards higher-level policies.</p> <p></p> <p>Here's a brief overview of each layer in our Clean Architecture:</p>"},{"location":"design/facade/overview/#domain-layer","title":"Domain Layer","text":"<p>The Domain Layer, or Entities, encapsulates the most general and high-level rules of a system. It can be an object with methods, or it can be a set of data structures and functions. In our case, we primarily use models from the FHIR library as our domain models. Other domain-related models are located in our Core layer.</p>"},{"location":"design/facade/overview/#application-layer","title":"Application Layer","text":"<p>The Application Layer, or Use Cases, defines the specific business rules of an application. They encapsulate all the use cases of a system, which can be initiated by either a user, an external system, or an event like a scheduled job.</p>"},{"location":"design/facade/overview/#infrastructure-layer","title":"Infrastructure Layer","text":"<p>The Infrastructure Layer, or Interface Adapters, contains all the implementations of the interfaces defined in the Application Layer. This layer is responsible for communicating with external systems, such as databases, web services, http clients, etc.</p>"},{"location":"design/facade/overview/#presentation-layer","title":"Presentation Layer","text":"<p>The Presentation Layer is the outermost layer of a system. This layer is responsible for providing a user interface to the system, in our case a REST API, this layer is also responsible for handling any external events that are triggered by other systems, and provide a way to communicate with the Application Layer as well for an Api configuration and startup.</p>"},{"location":"design/facade/overview/#result-object-pattern","title":"Result Object Pattern","text":"<p>This project uses the Result Object Pattern to handle errors and exceptions. The Result Object Pattern is a functional programming pattern that allows us to handle errors and exceptions in a more explicit way than using try/catch blocks. It also allows us to avoid returning null values from methods. Instead, we return a Result object that contains either the value, an exception or neither (representing null). We've created a custom Result struct that we use throughout our application.</p> <p>Example usage from the <code>DataHubFhirClient</code> class:</p> <pre><code>public async Task&lt;Result&lt;T&gt;&gt; GetResource&lt;T&gt;(string resourceId) where T : Resource\n    {\n        var resourceType = ModelInfo.GetFhirTypeNameForType(typeof(T));\n        logger.LogInformation(\"Fetching resource {ResourceType}/{ResourceId} from FHIR service.\", resourceType, resourceId);\n\n        try\n        {\n            var response = await dataHubFhirClient.ReadAsync&lt;T&gt;($\"{resourceType}/{resourceId}\");\n            return response; // The return value will be implicitly converted to a Result&lt;T&gt; object with success status\n        }\n        catch (FhirOperationException ex) when (ex.Status == HttpStatusCode.NotFound)\n        {\n            logger.LogDebug(\"Resource {ResourceType}/{ResourceId} not found in FHIR service.\", resourceType, resourceId);\n            return ex; // The exception will be implicitly converted to a Result&lt;T&gt; object with failure status\n        }\n        catch (Exception ex)\n        {\n            logger.LogError(\"Error fetching resource {ResourceType}/{ResourceId} from FHIR service: {ErrorMessage}\", resourceType, resourceId, ex.Message);\n            return ex; // The exception will be implicitly converted to a Result&lt;T&gt; object with failure status\n        }\n    }\n</code></pre> <p>If a method returns a null value, it will be implicitly converted to a corresponding <code>Result</code> object with neither a value nor an exception. This represents the null value, and can be checked using the <code>IsNull</code> property.</p> <p>For methods with a void return type, we use the <code>Result</code> struct instead of <code>Result&lt;T&gt;</code>. For example:</p> <pre><code>public Result doSomething()\n{\n    try\n    {\n        // Do something...\n        return Result.Success();\n    }\n    catch (Exception ex)\n    {\n        return ex; // Can also return Result.Failure(ex)\n    }\n}\n</code></pre>"},{"location":"design/facade/overview/#centralized-project-and-package-configuration","title":"Centralized Project and Package Configuration","text":"<p>In our solution, we use <code>Directory.Build.props</code> and <code>Directory.Packages.props</code> files to centralize the configuration of our projects and NuGet packages. These files are located at the root of our solution and are automatically imported by all <code>.csproj</code> files. This helps us maintain consistency across our projects and manage our NuGet packages more efficiently.</p>"},{"location":"design/facade/overview/#directorybuildprops","title":"Directory.Build.props","text":"<p>The <code>Directory.Build.props</code> file is used to define common MSBuild properties that are shared across all projects. This includes properties such as the target framework, nullable reference types setting, and others. By defining these properties in a central location, we ensure that all our projects are using the same settings, which helps us maintain consistency and avoid duplication.</p> <p>Here's an example of what our <code>Directory.Build.props</code> file looks like:</p> <pre><code>&lt;Project&gt;\n    &lt;PropertyGroup&gt;\n        &lt;TargetFramework&gt;net8.0&lt;/TargetFramework&gt;\n        &lt;Nullable&gt;enable&lt;/Nullable&gt;\n        &lt;ImplicitUsings&gt;enable&lt;/ImplicitUsings&gt;\n        &lt;InvariantGlobalization&gt;true&lt;/InvariantGlobalization&gt;\n\n        &lt;AnalysisMode&gt;Recommended&lt;/AnalysisMode&gt;\n        &lt;CodeAnalysisTreatWarningsAsErrors&gt;true&lt;/CodeAnalysisTreatWarningsAsErrors&gt;\n    &lt;/PropertyGroup&gt;\n&lt;/Project&gt;\n</code></pre> <p>we're setting the target framework to .NET 8.0, enabling nullable reference types and implicit usings, and setting the globalization invariant to true, in analysis mode we are using the recommended rules and we are treating warnings as errors.</p>"},{"location":"design/facade/overview/#directorypackagesprops","title":"Directory.Packages.props","text":"<p>The <code>Directory.Packages.props</code> file is used to manage our NuGet packages centrally. This file contains <code>PackageVersion</code> items for all the NuGet packages that are used across our projects. By defining these items in a central location, we can manage our NuGet packages versions more efficiently and ensure that all our projects are using the same package versions. in addition to that, this is a great preparation for using tools like Paket for dependency management.</p> <p>Here's an example of the Common section in our <code>Directory.Packages.props</code> file :</p> <pre><code>&lt;Project&gt;\n    &lt;PropertyGroup&gt;\n        &lt;ManagePackageVersionsCentrally&gt;true&lt;/ManagePackageVersionsCentrally&gt;\n    &lt;/PropertyGroup&gt;\n\n    &lt;ItemGroup Label=\"Common\"&gt;\n        &lt;PackageVersion Include=\"Microsoft.Extensions.Http\" Version=\"8.0.0\"/&gt;\n        &lt;PackageVersion Include=\"Microsoft.Extensions.Logging.Abstractions\" Version=\"8.0.0\"/&gt;\n        &lt;PackageVersion Include=\"Microsoft.Extensions.DependencyInjection.Abstractions\" Version=\"8.0.0\"/&gt;\n        &lt;PackageVersion Include=\"NEL.MESH\" Version=\"1.0.0.8\"/&gt;\n    &lt;/ItemGroup&gt;\n&lt;/Project&gt;\n</code></pre> <p>we're defining <code>PackageVersions</code> for nuget packages that are used across all our projects. This includes packages such as <code>Microsoft.Extensions.Http</code>, <code>Microsoft.Extensions.Logging.Abstractions</code>, <code>Microsoft.Extensions.DependencyInjection.Abstractions</code>, and <code>NEL.MESH</code>. This way Projects that use both of these packages will use the same version.</p>"},{"location":"design/facade/overview/#csproj-files","title":".csproj files","text":"<p>In your <code>.csproj</code> files, you don't need to specify the versions of the packages or the common properties that you have defined in <code>Directory.Build.props</code> and <code>Directory.Packages.props</code>. The versions and properties will be automatically applied to all projects in your solution. Here's an example of how a <code>.csproj</code> file might look:</p> <pre><code>&lt;Project Sdk=\"Microsoft.NET.Sdk\"&gt;\n\n  &lt;ItemGroup&gt;\n    &lt;PackageReference Include=\"Microsoft.Extensions.Http\" /&gt;\n    &lt;PackageReference Include=\"Microsoft.Extensions.Logging.Abstractions\" /&gt;\n    &lt;PackageReference Include=\"Microsoft.Extensions.DependencyInjection.Abstractions\" /&gt;\n    &lt;PackageReference Include=\"NEL.MESH\" /&gt;\n  &lt;/ItemGroup&gt;\n\n&lt;/Project&gt;\n</code></pre>"},{"location":"developer/configuration-structure/","title":"Configuration Structure","text":"<p>There are five main configuration files in the <code>api</code> project. These are:</p> <ol> <li>appsettings.json</li> <li>appsettings.Local.json</li> <li>appsettings.Integration.json</li> <li>appsettings.Development.json</li> <li>appsettings.Production.json</li> </ol> <p>The <code>appsettings.json</code> file contains the default/common configuration for the application, and the contains placeholders for the environment specific configuration. The other files are used to override the default configuration based on the environment the application is running in. The <code>appsettings.Local.json</code> file is used for local development, and the <code>appsettings.Integration.json</code> file is used for integration testing. The <code>appsettings.Development.json</code> file is used for development environment, and the <code>appsettings.Production.json</code> file is used for production and staging. The following setting are being overridden by terraform:</p> <ul> <li>\"Pds__Fhir__Authentication__UseCertificateStore\"</li> <li>\"Pds__Fhir__Authentication__CertificateThumbprint\"</li> <li>\"DataHubFhirServer__Authentication__Scope\"</li> <li>\"ApplicationInsightsAgent_EXTENSION_VERSION\"</li> <li>\"AzureStorageConnectionString\"</li> <li>\"AzureTableStorageCache__Endpoint\"</li> <li>\"DataHubFhirServer__BaseUrl\"</li> <li>\"DataHubFhirServer__TemplateImage\"</li> <li>\"Ndop__Mesh__MailboxId\"</li> <li>\"Ndop__Mesh__MailboxPassword\"</li> <li>\"Ndop__Mesh__RootCertificate\"</li> <li>\"Ndop__Mesh__ClientCertificate\"</li> <li>\"Ndop__Mesh__SubCertificate\"s</li> </ul>"},{"location":"developer/development-guide/","title":"Development Guidelines","text":"<p>Before you start developing, please read the following guidelines.</p>"},{"location":"developer/development-guide/#data-initialisation","title":"Data Initialisation","text":"<p>In order to have data in your FHIR server when it comes up, you will need to run the data initialization which includes:</p> <ol> <li>Insertion of FHIR profiles - upload a FHIR implementation guide profile package to a FHIR server</li> <li>Insertion of ICS organisations data - insert the ICS organisations jsons in the <code>/scripts/organisations/data</code> directory.</li> </ol> <p>To do so, you can run the data-init container with the FHIR server URL as an argument:</p> <pre><code>FHIR_SERVER_URL=http://localhost:8080/ docker-compose -f docker/docker-compose.yml up data-init\n</code></pre> <p>You can also specify the package name and version as arguments to override defaults:</p> <pre><code>FHIR_SERVER_URL=http://localhost:8080/ PACKAGE_ID=fhir.r4.ukcore.stu3.currentbuild PACKAGE_VERSION=0.0.8-pre-release docker-compose -f docker/docker-compose.yml up data-init\n</code></pre> <p>Or supply a server token if the server requires authentication:</p> <pre><code>FHIR_SERVER_URL=http://localhost:8080/ FHIR_SERVER_TOKEN=&lt;auth-bearer-token&gt; docker-compose -f docker/docker-compose.yml up data-init\n</code></pre> <p>This container is intended to be used locally for development purposes, like testing the validity of converted resources, and also for inserting FHIR data to a FHIR server in a CD pipeline, on a remote FHIR server.</p>"},{"location":"developer/development-guide/#liquid-templates","title":"Liquid Templates","text":"<p>The liquid templates which define how source data should be converted into FHIR are stored in the <code>./templates</code> directory of the repository. In order for data conversion operations to execute you need to build a docker container containing the templates, and push it to Azure Container Registry.</p> <p>A docker image is supplied which performs this task.</p> <p>First set the tag which will be applied to the container in the <code>.env</code> file, within the <code>./docker</code> directory. You also need to update your local appsettings with the tag value before run running the solution, and running the tests.</p> <p>Next build the container with the following command:</p> <p><code>docker-compose -f docker/docker-compose.yml build templates-pusher</code></p> <p>And finally run the container with the following command, which will push the container to the ACR:</p> <p><code>docker-compose -f docker/docker-compose.yml run templates-pusher</code></p> <p>:information_source: You need to rebuild and run the container if any changes are made to the templates.</p>"},{"location":"developer/development-guide/#running-the-application-with-its-dependencies","title":"Running the application with its dependencies","text":"<p>Alternatively to running each service individually (as described above), you can run all the required services using the script <code>start-clean.sh</code>:</p> <pre><code>bash ./docker/start-clean.sh\n</code></pre> <p>This process is also described in the Getting Started guide and it will not only run the data initialization and the templates pusher, but also the FHIR server, the SQL Server database for the FHIR server, and the Azurite Azure Storage Emulator.</p>"},{"location":"developer/integration-guide/","title":"Integration Guide","text":""},{"location":"developer/integration-guide/#connecting-to-healthcare-exchange","title":"Connecting to Healthcare Exchange","text":"<p>Healthcare Exchange supports receiving HL7 2.x, HL7 FHIR, and CSV messages via the HTTPS transport protocol.</p> <p>Healthcare Exchange does not support sending HL7 messages directly to a TCP port.</p> <p>MLLP is not used. The HL7 message is sent in plaintext and should not be encoded.</p> <p>The $ingest endpoint is:</p> <p>{agreed FQDN}{:agreed port}/$ingest</p> <p>Development environment:</p> <p>http://localhost:8000/$ingest</p> <p>Production:</p> <p>https://{FQDN}/$ingest</p>"},{"location":"developer/integration-guide/#authentication-tbd","title":"Authentication (TBD)","text":"<p>Mutual TLS should be used to guarantee the security and authenticity of both the client (receiving integration engine) and the server (Healthcare Exchange). Certificates will need to be exchanged in advance and deployed using the relevant tool for your environment, typically this is keytool for an integration based on a java virtual machine (JVM) or by adding to the windows certificate store.</p> <p>Messages are sent in plaintext and are not encoded or encrypted, instead the underlying connection is encrypted and considered to be secure providing mutual TLS is enabled.</p> <p>Authentication is achieved using OAuth2.  The OAuth endpoint will be provided through API management.</p> <p>What is OAuth2?</p> <p>How do I implement OAuth2 in my integration engine flow:</p> <p>Channel based method (Mirth)</p> <p>JWT (Mirth)</p> <p>The Service Principal will be issued upon successful authentication with the $ingest/auth service (TBD). The Service Principal* is the token which is used for successive requests to the $ingest service.</p>"},{"location":"developer/integration-guide/#authorisation","title":"Authorisation","text":"<p>The Service Principal must be registered before the $ingest endpoint can be used. The following roles/permissions must be allocated to the Service Principal</p> <p>*Connect as organisation</p> <p>Send as organisation</p> <p>Send message category*</p>"},{"location":"developer/integration-guide/#the-ingest-request","title":"The $ingest request","text":"<p>The following variables must be configured to be sent in the HTTP header:</p> <p>source-domain example: emergency-care</p> <p>organisation-code example: r0d</p> <p>data-type example: Hl7v2</p> <p>The capitalisation of the data-type must match Hl7v2 exactly</p> <p>The source-domain must exist within Healthcare Exchange, here we specify the emergency-care source-domain.</p> <p>Organisation codes match those provided by ODS.</p> <p>The data-type parameter specifies the message type being passed.</p> <p>A Content-Type of text/plain is required. The charset option is not required, but if used should be set to utf-8.</p> <p>The following example demonstrates setting up an integration flow using Mirth, but the same principles should apply to any other integration engine.</p> <p></p> <p></p>"},{"location":"developer/integration-guide/#viewing-logs","title":"Viewing logs","text":"<p>Running docker containers:</p> <p>docker ps -a</p> <p>Output the tail of the log:</p> <p>docker logs --follow</p>"},{"location":"developer/integration-guide/#deploying-the-channel","title":"Deploying the channel","text":"<p>(Mirth)</p> <p>In the dashboard view, use filter to find your channel</p> <p>Right click, choose undeploy channel</p> <p>Return to the channels view, use filter to find your channel</p> <p>Right click, choose deploy</p>"},{"location":"developer/integration-guide/#sending-a-message","title":"Sending a message","text":"<p>(Mirth)</p> <p>In the dashboard view, double click on the channel</p> <p>Right click on the message pane</p> <p>Click Send Message</p> <p>Paste a HL7 message into the edit control, or generate a suitable message using the generator</p> <p>Click Process Message</p>"},{"location":"developer/integration-guide/#viewing-the-response","title":"Viewing the response","text":"<p>In the dashboard view</p> <p>Click on raw or encoded to see the message</p> <p>Click on sent to see the message and HTTP headers sent</p> <p>Click on response to view the response</p> <p>MSH|^~\\&amp;|DEX|QVV|emergency-care|r0d|20240213144932||ACK|30d831e4-7f40-4bb9-9322-a42bfbc28bc5|P|2.4</p> <p>MSA|AE|8457239|{\"resourceType\":\"OperationOutcome\",\"id\":\"f7f8910f-b56c-4f97-a668-692e03f049c8\",\"meta\":{\"lastUpdated\":\"2024-02-13T14:49:32.5812032+00:00\"},\"issue\":[{\"severity\":\"error\",\"code\":\"unknown\",\"diagnostics\":\"\"}]}</p>"},{"location":"developer/integration-guide/#using-fhir-to-checkretrieve-the-stored-resource","title":"Using FHIR to check/retrieve the stored resource","text":"<p>The api documentation can be accessed using swagger.  Swagger exposes all direct interfaces and those proxied through the facade to the FHIR server.  These interfaces can be used directly via swagger.</p> <p>https://{agreed FQDN}/swagger/index.html</p> <p>The following FHIR query should return the stored resource:</p> <p>Production:</p> <p>https://{agreed FQDN}/Patient?_id=0000000000</p>"},{"location":"developer/pipelines/","title":"Pipelines","text":""},{"location":"developer/pipelines/#pr-pipeline-pr-pipelineyaml","title":"PR Pipeline (<code>pr-pipeline.yaml</code>)","text":""},{"location":"developer/pipelines/#overview","title":"Overview","text":"<p>This pipeline is triggered on pull requests to the <code>main</code> branch and consists of two stages: Static Code Analysis and Services Build Verification Tests.</p>"},{"location":"developer/pipelines/#stages","title":"Stages","text":""},{"location":"developer/pipelines/#1-static-code-analysis","title":"1. Static Code Analysis","text":"<ul> <li> <p>Description: Executes static code analysis on the codebase.</p> </li> <li> <p>Jobs:</p> </li> <li>RunAnalysis:<ul> <li>Steps:</li> <li>Uses a template (<code>static-code-analysis.yaml</code>) for executing static code analysis.</li> </ul> </li> </ul>"},{"location":"developer/pipelines/#2-services-build-verification-tests","title":"2. Services Build Verification Tests","text":"<ul> <li> <p>Description: Performs Build Verification Tests (BVT) for services and executes End-to-End (E2E) tests.</p> </li> <li> <p>Jobs:</p> </li> <li>BVT:<ul> <li>Strategy:</li> <li>Builds and tests the specified service (e.g., <code>api</code>) with a coverage gate of 0% ATM.</li> <li>Steps:</li> <li>Uses a template (<code>bvt.yaml</code>) with parameters for service name, project folder, and coverage gate.</li> <li>E2ETests:</li> <li>Dependencies: Depends on the completion of the BVT job.</li> <li>Steps:<ul> <li>Uses a template (<code>e2e-tests.yaml</code>) for executing End-to-End tests.</li> </ul> </li> </ul> </li> </ul>"},{"location":"developer/pipelines/#ci-pipeline-ci-pipelineyaml","title":"CI Pipeline (<code>ci-pipeline.yaml</code>)","text":"<p>This pipeline is triggered on changes to the <code>main</code> branch and has the same structure of the PR pipeline with Static Code Analysis and Services Build Verification Tests stages. In an upcoming user story we will also package the service and publish it to a private Azure Container Registry (ACR).</p>"},{"location":"developer/pipelines/#bvt-template-bvtyaml","title":"BVT Template (<code>bvt.yaml</code>)","text":""},{"location":"developer/pipelines/#overview_1","title":"Overview","text":"<p>This template defines steps for Build Verification Tests (BVT) for a specified service.</p>"},{"location":"developer/pipelines/#parameters","title":"Parameters","text":"<ul> <li>SERVICE_NAME: The name of the service to be tested (should be similar to the name in <code>docker-compose.yml</code>)</li> <li>PROJECT_FOLDER: The folder containing the service project.</li> <li>COVERAGE_GATE: The coverage gate percentage.</li> </ul>"},{"location":"developer/pipelines/#steps","title":"Steps","text":"<ul> <li>Build:</li> <li> <p>Builds the specified service using Docker Compose.</p> </li> <li> <p>Unit Tests:</p> </li> <li> <p>Executes unit tests for the service.</p> </li> <li> <p>Integration Tests:</p> </li> <li>Runs integration tests using Docker Compose, and if a <code>start.sh</code> script exists, it is executed before running tests.</li> <li> <p>the <code>start.sh</code> script is used to start the service and any dependencies (e.g., db, local fhir service) before running tests.</p> </li> <li> <p>Publish Test Results:</p> </li> <li> <p>Publishes test results in XUnit format.</p> </li> <li> <p>Unify Test Coverage Reports:</p> </li> <li> <p>Installs <code>dotnet-reportgenerator-globaltool</code> and generates a unified code coverage report in Cobertura format.</p> </li> <li> <p>Publish Code Coverage Results:</p> </li> <li> <p>Publishes the generated code coverage results.</p> </li> <li> <p>Quality Gate:</p> </li> <li> <p>The last section provides an example of adding a quality check based on test line coverage using the Build Quality Checks extension.</p> </li> </ul>"},{"location":"developer/pipelines/#static-code-analysis-template-static-code-analysisyaml","title":"Static Code Analysis Template (<code>static-code-analysis.yaml</code>)","text":""},{"location":"developer/pipelines/#overview_2","title":"Overview","text":"<p>This template includes steps for static code analysis, detecting secrets, and lint checks.</p>"},{"location":"developer/pipelines/#steps_1","title":"Steps","text":"<ul> <li>Detect Secrets Placeholder:</li> <li> <p>Placeholder for detecting secrets in the codebase.</p> </li> <li> <p>Lint check Placeholder:</p> </li> <li>Placeholder for lint checks.</li> </ul>"},{"location":"developer/pipelines/#advanced-security-tasks-optional-commented-out","title":"Advanced Security Tasks (Optional - Commented Out)","text":"<ul> <li>If using GitHub Advanced Security, the commented-out section provides tasks for CodeQL initialization, autobuild, dependency scanning, and code analysis.</li> </ul>"},{"location":"developer/pipelines/#e2e-tests-template-e2e-testsyaml","title":"E2E Tests Template (<code>e2e-tests.yaml</code>)","text":""},{"location":"developer/pipelines/#overview_3","title":"Overview","text":"<p>This template serves as a placeholder for executing End-to-End (E2E) tests.</p>"},{"location":"developer/pipelines/#steps_2","title":"Steps","text":"<ul> <li>Run E2E Placeholder:</li> <li>placeholder for running E2E tests.</li> </ul>"},{"location":"developer/pipelines/#cd-pipeline-cd-pipelineyaml","title":"CD Pipeline (<code>cd-pipeline.yaml</code>)","text":""},{"location":"developer/pipelines/#overview_4","title":"Overview","text":"<p>This pipeline is meant for the infrastructure and applications deployment of the solution. It can deploy to multiple environments, based on a parameter, and can be triggered manually for any git revision, tag, or commit, thus it gives the flexibility to deploy any version of the application to any environment.</p> <p>By default pipeline is triggered on a completion of a CI pipeline instance, for deploying a DEV environment, in a continuous deployment manner. Every environment name maps to a corresponding AzDO variable group, which holds the environment specific variables:</p>"},{"location":"developer/pipelines/#variable-group-keys","title":"Variable Group Keys","text":"<p>:warning: The list of keys below is incomplete and will be clarified shortly.</p>"},{"location":"developer/pipelines/#dev-secret","title":"dev-secret","text":"<ul> <li>AZURE_TENANT_ID: The Azure tenant id to deploy into.</li> <li>azureSubscriptionEndpoint: Specifies the Azure Resource Manager service connection. In our case, defined using workload identity federation with openid connect.</li> </ul>"},{"location":"developer/pipelines/#dev-variables","title":"dev-variables","text":"<ul> <li>apim_subnet_address_prefixes</li> <li>app_gateway_subnet_address_prefixes</li> <li>app_plan_sku</li> <li>app_plan_subnet_address_prefixes</li> <li>app_registration_id</li> <li>app_registration_owners</li> <li>app_registration_uri</li> <li>AZURE_SUBSCRIPTION_ID</li> <li>AZURE_TENANT_ID</li> <li>azureSubscriptionEndpoint</li> <li>location</li> <li>log_analytics_sku</li> <li>pds_fhir_certificate_name</li> <li>RG_DEX_GENERAL</li> <li>services_subnet_address_prefixes</li> <li>tf_status_storage_account</li> <li>vnet_address_space</li> </ul>"},{"location":"developer/pipelines/#stages_1","title":"Stages","text":"<ul> <li>Deploy infrastructure:</li> <li>Download swagger file for API Management endpoint definition (note that the CD pipeline references the CI pipeline 'Build and Publish Swagger Docs' stage)</li> <li>Add Agent IP to Key Vault vnet_- in order for the ADO agent to be able to access the key vault during deployment, the agent's IP address needs to be added to the key vault's firewall.<ul> <li>Create terraform tfvars file</li> <li>Apply Terraform Infrastructure as Code, using the tfvars file created in the previous step. Note that the tfstate file is stored in an Azure Storage Account, and the connection details are provided to Terraform via the <code>backend-config</code> parameters.</li> </ul> </li> <li>Deploy services:</li> <li>Build and push API container image to ACR (with both buildId and <code>latest</code> as tags)</li> <li>Deploy API container image to its dedicated App Service (using the buildId tag)</li> <li>Update Agent IP in App Gateway WAF policy</li> <li>Update FHIR Profiles in the FHIR service.</li> <li>Push Conversion Liquid Template OCI Images to ACR (with both buildId and <code>latest</code> as tags).</li> <li>Run smoke tests to verify the deployment.</li> </ul>"},{"location":"developer/quick-start-cloud-dev/","title":"Testing the service on the cloud","text":"<p>In order to make a request to the service (on cloud), you will need to perform the following steps.</p>"},{"location":"developer/quick-start-cloud-dev/#pre-requirements","title":"Pre requirements","text":"<ul> <li>A member of the subscription which you will use to host the Dex.</li> <li>The Dex solution deployed to the above Azure subscription.</li> <li>Azure CLI (You can install the Azure CLI)</li> </ul>"},{"location":"developer/quick-start-cloud-dev/#1-allow-your-ip","title":"1. Allow your IP","text":"<p>Application Gateway is the entrypoint to the service and is configured with a Web Application Firewall (WAF).\\ This WAF has a policy which allows certain IPs and blocks all others.\\ If you try to make a request to the public IP of Application Gateway without allowing your IP, then you will receive a <code>403 Forbidden</code> response. Use the script setup-dev-to-infra-access.sh to allow your IP to access the Application Gateway and the scm (kudo) site for the web app.</p> <pre><code># Setup environment variables\nexport CLIENT_ID=&lt;client (service principle) id&gt;\nexport SUBSCRIPTION_ID=&lt;subscription id&gt;\nexport TENANT_ID=&lt;tenant id&gt;\nexport CLIENT_SECRET=&lt;client (service principle) secret&gt;\nexport env=&lt;dev/staging/production&gt;\n\nsh ./scripts/setup-dev-to-infra-access.sh\n</code></pre> <p>If you prefer to do it manually, follow the steps below:\\  Make sure you're logged-in to the Azure subscription which you will use to host Dex.</p> <ul> <li>Go to Application Gateway (<code>agw-dex-dev</code>) via the Azure Portal.</li> <li>Next, go to Settings &gt; Web application firewall.</li> <li>Select the associated WAF policy (<code>agw-dex-dev-waf</code>)</li> <li>Go to Settings &gt; Custom rules</li> <li>Select the rule AllowAllForTrustedIP.</li> <li>Under the \"IP address or range\", paste your IP address. (Hint: You can find your IPv4 address here.)</li> <li>Now, when you make a request you should get the response <code>401 Unauthorized</code>.</li> </ul>"},{"location":"developer/quick-start-cloud-dev/#2-generate-a-token","title":"2. Generate a Token","text":"<ul> <li>Our Azure API Management service is configured with authentication and authorization. This means that you will need to provide an access token in order to be able to query the service, so that APIM can determine who you are and what you have access to.</li> <li>To generate an access token, use the Azure CLI:</li> </ul> <pre><code>az login\naz account get-access-token --resource api://8d514f44-357e-444f-8459-51a595e698d2\n</code></pre> <p>Note: the resource ID is the URI of the client application registration, which you will need to be added to.</p>"},{"location":"developer/quick-start-cloud-dev/#3-optional-inspect-the-token","title":"3. [Optional] Inspect the token","text":"<ul> <li>Once you have generated a token, you can use a tool such as jwt.ms to decode it. It will tell you information such as the audience, when it expires, the name of the user, and their roles.</li> <li>This can be helpful for checking that the token values are what is expected.</li> </ul>"},{"location":"developer/quick-start-cloud-dev/#4-make-a-request","title":"4. Make a request","text":"<p>[PLACEHOLDER - TO DO] Frances to explain how to get the correct IP address and how to make a request to the service.</p> <ul> <li>Now, provide an Authorization header for your request and use the token.</li> <li>You should receive the response <code>200 OK</code>.</li> </ul> <p>An example, using Postman would be: </p> <ul> <li>This uses the Public IP of Application Gateway in the GET request.</li> <li>It calls the endpoint <code>/sessions</code> which is defined in Azure API Management.</li> <li>It uses the Authorization header, with type Bearer Token and the token value returned using the Azure CLI.</li> <li>The response is <code>200 OK</code> along with details of the sessions from the API.</li> </ul>"},{"location":"developer/source-integration-setup/","title":"Source Integration Setup","text":""},{"location":"developer/source-integration-setup/#deciding-between-push-or-pull-integration","title":"Deciding between Push or Pull integration","text":"<p>Healthcare Data Exchange integrates with multiple consumers and providers, as described in the Architecture page. The data from the providers can be either pushed to the platform or pulled from the sources. This data can be in multiple formats.</p> <p>The decision to use a push or pull strategy depends on whether the data providers are capable of actively pushing data to the platform. If push is not possible, the data needs to be pulled from the sources.</p>"},{"location":"developer/source-integration-setup/#pull","title":"Pull","text":"<p>Data is ingested into the platform by pulling from the sources. This can be done for example by reading a message from a MESH mailbox or with RESTful API calls.</p>"},{"location":"developer/source-integration-setup/#push","title":"Push","text":"<p>Data is pushed to the platform by accepting REST HTTP requests in FHIR or HL7 formats, or streaming CSV files in the request body. You can add support for other formats as described in the Adding a new template section. This strategy is useful when the data providers are capable of actively pushing data. In case a source is not able to push files in HTTP requests, an 'agent' application might be deployed on the client side, that will push the data to the platform.</p>"},{"location":"developer/source-integration-setup/#data-normalization","title":"Data Normalization","text":"<p>Regardless of the data ingestion method, for the data to be stored in the FHIR DB and be queryable by consumers, the data needs to be normalized into a unified FHIR format. This process is further explained in the Adding a new template section.</p>"},{"location":"developer/source-integration-setup/#authentication-and-authorization","title":"Authentication and Authorization","text":"<p>Data providers need to be authenticated and authorized to interact with the platform. The platform uses Azure Entra for authentication and authorization.</p> <p>When data providers are pushing data to the platform, they need to be authenticated and authorized against our system. They need to be registered in Entra and have the role of data provider to be able to push data.</p> <p>When the platform is pulling data from the data providers, our system needs to be authenticated and authorized against their system.</p> <p>Further detail is provided in the Authentication and Authorization section (TO DO - Frances).</p>"},{"location":"developer/source-integration-setup/#deciding-on-data-persistence","title":"Deciding on data persistence","text":"<p>The second decision to make is whether the data needs to be persisted in the FHIR DB. If the data needs to be persisted in the FHIR DB, the data needs to go through conversions and mappings to be transformed into a unified FHIR format. If the data has to be persisted in the original format, the data can be stored in Azure Blob Storage. You will also need to decide on the retention period for the data.</p>"},{"location":"developer/source-integration-setup/#adding-a-new-template","title":"Adding a new template","text":"<p>The following instructions apply if you decided to persist the data in the FHIR DB and need to convert it to FHIR format. For more information about templates and data mapping see here</p>"},{"location":"developer/source-integration-setup/#adding-a-converter-if-the-source-is-csv","title":"Adding a converter (if the source is CSV)","text":"<p>If the source data is in CSV format, you will need to create a custom converter from CSV to JSON: <code>src/Core/Converters</code>. You can use the existing converters as a reference, for example <code>src/Core/Converters/PdsMeshCsvToJsonConverter.cs</code>.</p>"},{"location":"developer/source-integration-setup/#adding-a-liquid-template","title":"Adding a Liquid template","text":"<p>Once you have converted the source data to JSON or if the source data is in another format (rather than CSV), you will need to create a Liquid template to convert the source data to FHIR.</p> <p>In order to integrate a new source system, you need to create a new template. The template defines how source data should be converted into FHIR. The templates are written in Liquid templating language and make use of custom filters.</p> <p>The liquid templates are stored in the <code>./templates</code> directory of the repository. To add a new template, create a folder within the directory with the name of the source system (organisation code e.g. rbd). Within this folder, create folders representing each source domain (e.g. emergency-care) and within each source domain, further subdivide the templates into folders representing the source data type (e.g. hl7v2).</p>"},{"location":"developer/source-integration-setup/#adding-tests-for-the-new-template","title":"Adding tests for the new template","text":"<p>The tests for the templates are stored in the <code>./tests</code> directory of the repository. To add further tests, create a new folder structure (corresponding to the structure within the <code>./templates</code> directory) within the <code>Templates.Tests/input</code> directory, and add some examples of source data. Add corresponding folders into the <code>Templates.Tests/output</code> directory which will contain the files with expected FHIR output in .json format after the conversion is complete. See the examples in the <code>./tests/Templates.Tests</code> directory for further guidance.</p>"},{"location":"developer/source-integration-setup/#pushing-the-new-template-to-azure-container-registry","title":"Pushing the new template to Azure Container Registry","text":"<p>In order for data conversion operations to execute, you need to build a docker container containing the templates, and push it to Azure Container Registry. A docker image is supplied which performs this task.</p> <p>First, set the tag which will be applied to the container in the <code>.env</code> file, within the <code>./docker</code> directory. You also need to update your local <code>appsettings.json</code> file (found within the root folder) with the tag value before running the solution, and running the tests.</p> <p>Next build the container with the following command:</p> <p><code>docker-compose -f docker/docker-compose.yml build templates-pusher</code></p> <p>And finally run the container with the following command, which will push the container to the ACR:</p> <p><code>docker-compose -f docker/docker-compose.yml run templates-pusher</code></p> <p>:information_source: You need to rebuild and rerun the container if any changes are made to the templates.</p>"},{"location":"developer/source-integration-setup/#create-a-new-integration-and-connect-it-to-the-current-code","title":"Create a new integration and connect it to the current code","text":"<p>Our integrations are separated into two main parts - Infrastructure and Core. The infrastructure part is responsible for importing the data from the source, and the core part is responsible for orchestrating the data flow, converting the data to the FHIR format, and persisting it.</p> <p>Each Integration has 3 main parts:</p> <ol> <li>API - responsible for invoking or exposing the integration. Exposed operations names are prefixed with the $    character,    to denote RPC-like additions as per the FHIR spec.</li> <li>Infrastructure - responsible for importing the data from the source.</li> <li>Core - responsible for orchestrating the data flow, converting the data to the FHIR format, and persisting it.</li> </ol> <p>Note: The separation and design of the integrations code is based on the common web application architectures - clean architecture design.</p>"},{"location":"developer/source-integration-setup/#example","title":"Example","text":"<p>We can see the ODS integration as an example. The ODS integration is importing NHS organisations data from different regions across the UK. Each region has its own CSV file, and the data is pulled into the platform using a scheduler.</p> <pre><code>graph TD\n;\n    OdsCsvDownloadBackgroundService.cs --&gt;|1| OdsService.cs\n    OdsService.cs --&gt;|2| OdsCsvDownloadClient.cs\n    OdsService.cs --&gt;|3| OdsCsvIngestionStrategy.cs\n    OdsCsvIngestionStrategy.cs --&gt;|4| OdsCsvToJsonConverter.cs\n    OdsCsvIngestionStrategy.cs --&gt;|5 + 6| DataHubFhirClient.cs\n\n</code></pre> <ol> <li> <p>The first step is defining the type of the integration. In this case, the integration is a pull integration.    The data is pulled from the source using a scheduler. The scheduler is in the <code>src/Api</code> folder and triggers the    entire process.    <code>OdsCsvDownloadBackgroundService.cs</code>    is the background service that is responsible for invoking the ingestion, which is implemented in    the <code>OdsService.cs</code>.</p> </li> <li> <p>Next we need an orchestrator. This will be responsible for orchestrating the data flow. The orchestration logic can    be found in    the <code>OdsService.cs</code>    file.    This file is responsible for:</p> <ol> <li>Pulling the data from the source.</li> <li>Converting the data.</li> <li>Saving the data.</li> </ol> </li> <li> <p>To import data from a new source, an integration should be created in the <code>src/Infrastructure/&lt;INTEGRATION_NAME&gt;</code>    folder.    We will start by looking at the infrastructure and data import.    The ODS Infrastructure folder is separated into 2 folders:</p> <ol> <li><code>Clients</code> - This folder contains the clients that are responsible for pulling the data from the source.</li> <li><code>Configuration</code> - This folder contains the classes for binding the integration's configuration.</li> </ol> </li> <li> <p>Finally, we would like to have a way to convert and save the data into the FHIR Service. In    the <code>src/Core/&lt;INTEGRATION_NAME&gt;</code> folder, we will have the strategy and converters for the integration.    For ODS Integration, we have    the <code>OdsCsvIngestionStrategy.cs</code>    file. This is responsible for converting the CSV data to JSON and sending the data to the FHIR Service. This is    separated into 2 parts:</p> <ol> <li><code>Converters</code> - This folder contains the strategies for converting the    data. <code>OdsCsvToJsonConverter.cs</code>    is the converter that is responsible for converting the CSV data to JSON.</li> <li><code>DataHubFhirClient.cs</code> -    This is the client used to communicate with the Data Hub FHIR server and is performing two actions:<ol> <li>Converting the data using the Liquid templates and the $convert-data endpoint from JSON to FHIR.</li> <li>Saving the data into the FHIR server.</li> </ol> </li> </ol> </li> </ol> <p>Once you have created the integration, you need to connect it to the current code and register the created services. For both Core and Infrastructure, you need to add a DependencyInjection file in <code>src/Core/&lt;INTEGRATION_NAME&gt;</code> and <code>src/Infrastructure/&lt;INTEGRATION_NAME&gt;</code> respectively. As an example, see <code>ODS Core DependencyInjection.cs</code> and <code>ODS Infrastructure DependencyInjection.cs</code> files.</p> <p>Don't forget to add tests to the tests folders for the new integration.</p> <p>Note: You can also see the rest of the integrations (Ndop, Pds and DataHub) in the respective folders are built in the same way.</p>"},{"location":"developer/source-integration-setup/#defining-configuration-for-a-new-integration","title":"Defining configuration for a new integration","text":"<p>TBD (currently we do not have a unified configuration file for the integrations, but this will be added in the future)</p>"},{"location":"developer/swagger-file/","title":"Swagger File","text":"<p>As part of the API Management, we have a Swagger file that describes the API. This file is used to generate the API documentation and to create the API in the API Management. The Swagger file is not stored in the repository, but is generated by the CI pipeline and published as an artifact. The file is generated by the <code>Build and Publish Swagger Docs</code> job in the CI pipeline and is published as an artifact named <code>dex-swagger.json</code>. The file is then used by the CD pipeline and terraform to create the API in the API Management together with the relevant policies. In order to generate the swagger file for local testing, you can run the following script:</p> <pre><code>bash ./.pipelines/scripts/generate-swagger.sh infrastructure/services/api_management\n</code></pre> <p>This script will generate the swagger file and store it in the <code>infrastructure/services/api_management</code> folder. The file can then be used to create the API in the API Management via terraform.</p>"},{"location":"developer/version-control/","title":"Version Control","text":""},{"location":"fhir-store/ids-and-references/","title":"IDs and References","text":"<p>The purpose of this design is to describe how we will compute Resource IDs during the FHIR ingestion process.</p>"},{"location":"fhir-store/ids-and-references/#background","title":"Background","text":"<p>To reduce the complexity of data consumption, we have agreed to persist resources with Literal References. Resources will be identified and addressed by their URL.</p> <p>The data we ingest uses Logical References. For example, referencing a <code>Organisation</code> using the ODS Code. The referenced resource may not exist within the ingested bundle. Without computing a deterministic ID it would be required to perform multiple lookups on the FHIR server during ingestion.</p>"},{"location":"fhir-store/ids-and-references/#terminology","title":"Terminology","text":"<ol> <li>Source Organisation - The organisation that is sending the data to ingest.</li> <li>Source Domain - The business domain where the source data originates from.</li> <li>Mapping Config - Configuration that defines what Mapping Template to use for a given input.</li> <li>Mapping Template - A Liquid template that defines how <code>$convert</code> to FHIR.</li> </ol>"},{"location":"fhir-store/ids-and-references/#ingestion-overview","title":"Ingestion Overview","text":""},{"location":"fhir-store/ids-and-references/#process-steps","title":"Process Steps","text":""},{"location":"fhir-store/ids-and-references/#step-1-ingest-request","title":"Step 1 <code>$ingest</code> request","text":"<p>Source data is sent to the Hl7v2 <code>$ingest</code> endpoint. The following inputs are captured in the HTTP request headers:</p> <ul> <li><code>organisation-code</code></li> <li><code>source-domain</code></li> <li><code>data-type</code></li> </ul> <p>Together, <code>organisation-code</code> and <code>source-domain</code> are used later to scope the ID generation.</p>"},{"location":"fhir-store/ids-and-references/#step-2-create-convertdatarequest","title":"Step 2 Create <code>ConvertDataRequest</code>","text":"<p>Data Hub will create a <code>ConvertDataRequest</code> which encapsulates the necessary information for conversion:</p> <ul> <li><code>Input</code> - from request body</li> <li><code>TemplateInfo</code></li> <li><code>OrganisationCode</code>: from request header</li> <li><code>Domain</code>: from request header</li> <li><code>DataType</code>: from request header</li> <li><code>ResourceType</code></li> </ul>"},{"location":"fhir-store/ids-and-references/#step-34-convert-data-to-fhir","title":"Step 3/4 <code>$convert-data</code> to FHIR","text":"<p>Data Hub will send the <code>ConvertDataRequest</code> to the <code>$convert-data</code> endpoint, which returns a FHIR bundle based on the required Liquid template.</p> <p>The mapping process is responsible for generating the ID and Literal References for all resources.</p>"},{"location":"fhir-store/ids-and-references/#resource-id-generation","title":"Resource ID Generation","text":"<ol> <li>Extract identifiers from the input segment or field;</li> <li>Combine the identifiers with resource type and base ID (optional) as hash seed;</li> <li>Compute hash as output ID</li> </ol> <p>This is an example ID template for a <code>Organization</code>:</p> <pre><code>{{ OdsCode | generate_id_input: 'Organization', false | generate_uuid }}\n</code></pre> <p>This template would be used when mapping a new <code>Organization</code>:</p> <pre><code>{\n  \"resourceType\": \"Bundle\",\n  \"type\": \"transaction\",\n  \"entry\": [\n    {% for item in msg.orgs %}\n    {% evaluate orgId using 'ID/Organization' OdsCode: item.OdsCode -%}\n    {\n      \"resource\": {\n        \"resourceType\": \"Organization\",\n        \"id\": \"{{ orgId }}\",\n</code></pre> <p>The same template would be used to reference a <code>Organization</code>:</p> <pre><code>{% evaluate orgId using 'ID/Organization' OdsCode: ... -%}\n{% assign fullOrgId = orgId | prepend: 'Organization/' -%}\n{% include 'Reference/Organization' ID: encounterId, REF: fullOrgId -%}\n</code></pre> <p>Note: The complexity of ID generation is encapsulated in the <code>Id/Organization</code> template. For consistency, this should be referenced from all top-level templates.</p>"},{"location":"fhir-store/ids-and-references/#base-id","title":"Base ID","text":"<p>The <code>generate_id_input</code> filter includes a concept of \"base resource/base ID\". Base resources are independent entities, like <code>Patient</code>, <code>Organization</code>, <code>Device</code>, etc, whose IDs are defined as base ID. Base IDs could be used to generate IDs for other resources that relate to them. It helps enrich the input for hash and thus reduce ID collision. For example, a <code>Patient</code> ID is used as part of hash input for an <code>AllergyIntolerance</code> ID, as this resource is closely related with a specific patient.</p>"},{"location":"fhir-store/ids-and-references/#scoped-id","title":"Scoped ID","text":"<p>For many resource types, it will be required to scope their ID on the Source Organisation and/or Source Domain. In this scenario, these values can be hard-coded in the top-level template.</p> <p>This is an example ID template for a scoped <code>Organization</code>:</p> <pre><code>{% capture identifiers -%}\n    {{ SourceOrg }}_{{ SourceDomain }}_{{ OdsCode }}\n{% endcapture -%}\n{{ identifiers | generate_id_input: 'Patient', false | generate_uuid }}\n</code></pre> <p>This would be used when mapping a new scoped <code>Organization</code>:</p> <pre><code>{\n  \"resourceType\": \"Bundle\",\n  \"type\": \"transaction\",\n  \"entry\": [\n    {% for item in msg.orgs %}\n    {% evaluate orgId using 'ID/Organization' OdsCode: item.OdsCode, SourceOrg: 'NHS', SourceDomain: 'ODS' -%}\n    {\n      \"resource\": {\n        \"resourceType\": \"Organization\",\n        \"id\": \"{{ orgId }}\",\n</code></pre> <p>Because the ID is now scoped, the same inputs are required when referencing a <code>Organization</code> from a top-level template:</p> <pre><code>{% evaluate orgId using 'ID/Organization' OdsCode: ..., SourceOrg: 'NHS', SourceDomain: 'ODS'  -%}\n{% assign fullOrgId = orgId | prepend: 'Organization/' -%}\n{% include 'Reference/Encounter/Subject' ID: encounterId, REF: fullOrgId -%}\n</code></pre> <p>It is possible for the <code>SourceOrg</code> and <code>SourceDomain</code> to be different from the ingest request. For example:</p> <ul> <li>A01 from 'InPatient' Domain</li> <li>Encounter '0123' from Domain1: \"UCH_InPatient_0123\"</li> <li>A03 from 'ED' Domain</li> <li>Condition from 'ED' references Encounter from 'InPatient'. By default, our lookup would be incorrect: \"UCH_ED_0123\"</li> </ul>"},{"location":"fhir-store/ids-and-references/#step-5-resource-validation-and-post-processing","title":"Step 5 Resource validation and post-processing","text":"<p>See here.</p>"},{"location":"fhir-store/ids-and-references/#step-6-persist-bundle","title":"Step 6 Persist Bundle","text":"<p>In order to preserve the generated Resource IDs, the bundles are created with <code>PUT</code> requests, instead of <code>POST</code> requests. This bundle is sent to the FHIR Server for processing.</p>"},{"location":"fhir-store/ids-and-references/#exception-for-patient","title":"Exception for <code>Patient</code>","text":"<p>The <code>Patient</code> resource is considered a local cache of PDS. In PDS, the NHS Number is used as the resource ID. To remain consistent with the upstream data source, we will also use the NHS Number in the FHIR Server.</p> <p>As a result:</p> <ul> <li>Liquid mapping will reference <code>Patient</code> using the NHS Number: \"Patient/{NhsNumber}\"</li> <li>No post processing is required. If the <code>Patient</code> does not exist, it will be retrieved and persisted on read.</li> <li>The <code>PatientModule</code> handles <code>GET /Patient/&lt;NhsNumber&gt;</code> using the existing <code>SearchStrategy</code> logic (read locally, and if required read upstream and persist).</li> <li><code>Patient</code> will be persisted as-is from PDS.</li> </ul> <p>It is possible other resources may need to follow a similar pattern - typically when the referenced resource is owned by a national system. For example, E-Referral Service, or E-Prescribing Service.</p>"},{"location":"fhir-store/ids-and-references/#limitations-of-design","title":"Limitations of design","text":"<p>This design does include some limitations we should be aware of:</p> <ol> <li>Using a generated literal ID to reference a resource lacks a referential integrity check. For example, while the generated ID would be valid, the <code>Encounter</code> we are referencing may not exist. By design, the FHIR Server does not enforce referential integrity among FHIR resources.</li> <li>This is by-design for <code>Patient</code>. For this resource, we will persist-on-read.</li> <li>For other resources, we could iterate the bundle post-mapping and validate the references have integrity. This would require a number of requests to <code>GET /{ResourceType}/{Id}</code>.</li> <li>Mapping has no control over the referenced resource. Therefore, we are unable to update the bi-directional reference. For example, we could map a literal reference from a <code>Condition</code> to an <code>Encounter</code>. However, unless the <code>Encounter</code> is also in the bundle, we are unable to update the <code>Encounter</code> to also reference the <code>Condition</code>.</li> <li>A potential mitigation would be to iterate the bundle post-mapping and ascertain which referenced resources would also require updating. These could be appended to the bundle. This is considered non-trivial, and would likely require mapping configuration.</li> </ol>"},{"location":"fhir-store/overview/","title":"Overview","text":""},{"location":"fhir-store/profiles/","title":"Profiles","text":""},{"location":"infrastructure/custom-domain-and-tls-ssl/","title":"Custom Domains and TLS/SSL","text":""},{"location":"infrastructure/custom-domain-and-tls-ssl/#custom-domain","title":"Custom Domain","text":"<p>DNS records for DEX are managed by NHS England. See HSCN Domain Name System (DNS) for an overview of the domain name system on HSCN and the domain name system change request process.</p> <p>The table below lists the environment-specific domains that have been provisioned by NHS England, and their respective records:</p> Environment Hostname CNAME Dev dex-dev.nhsdorset.nhs.uk agw-pip-dex-dev.uksouth.cloudapp.azure.com Staging dex-staging.nhsdorset.nhs.uk agw-pip-dex-staging.uksouth.cloudapp.azure.com Production dex.nhsdorset.nhs.uk agw-pip-dex-prod.uksouth.cloudapp.azure.com <p>The CNAME record comes from the DNS label attached the App Gateway's public IP address.</p>"},{"location":"infrastructure/custom-domain-and-tls-ssl/#tlsssl","title":"TLS/SSL","text":"<p>TODO: Document process for provisioning, management, and rotating certs.</p>"},{"location":"infrastructure/custom-domain-and-tls-ssl/#storing-the-certificate-in-key-vault","title":"Storing the Certificate in Key Vault","text":"<p>When deploying the infrastructure using Terraform, the certificate is copied from the common Key Vault to an environment-specific Vault.</p> <p>The certificate should be located in the common Key Vault with the following name: \"dex-{{env}}-certificate-private\".</p> <p>Follow this tutorial to import a certificate in Key Vault using the portal.</p>"},{"location":"infrastructure/custom-domain-and-tls-ssl/#tlsssl-in-app-services-container-instance","title":"TLS/SSL in App Service's Container Instance","text":"<p>The DEX API runs as a custom container on the Azure App Service infrastructure.</p> <p>App Service terminates TLS/SSL at the front ends. That means that TLS/SSL requests never get to the app. It is not required, or recommended, to implement any support for TLS/SSL into your app or the container. See here for details.</p>"},{"location":"infrastructure/custom-domain-and-tls-ssl/#internal-traffic","title":"Internal Traffic","text":"<p>Our design requires all internal traffic to be encrypted using TLS/SSL. This is achieved by using Azure managed endpoints for the App Gateway, API Management, and the FHIR Server.</p> <p>Transport Layer Security (TLS) is a widely adopted security protocol designed to secure connections and communications between servers and clients.</p> Source Target Approach App Gateway API Management Gateway targets an Azure managed endpoint (*.azure-api.net). API Management Web API Backend targets an Azure managed endpoint (*.azurewebsites.net). API Management FHIR Server Backend targets an Azure managed endpoint (*.fhir.azurehealthcareapis.com). <p>Microsoft is responsible for provisioning, maintaining, and rotating TLS/SSL certificates for Azure managed endpoints.</p>"},{"location":"infrastructure/deployment/","title":"Deployment","text":""},{"location":"infrastructure/environments/","title":"Environments","text":""},{"location":"infrastructure/identity/","title":"Identity","text":"<p>The concept of identity enables the platform to recognise an entity which needs to perform a task on the platform. An entity could be could be a real person, another external system, or a component of the platform itself.</p> <p>An identity can be authenticated (e.g. via a username and password) so the platform knows who the entity is, and authorised so the platform can ensure the entity has the permissions required to perform the task it wants to perform.</p> <p>An identity will have an account, and the account will be assigned a number of roles which define the permissions it holds to perform tasks.</p>"},{"location":"infrastructure/identity/#accounts","title":"Accounts","text":"<p>Accounts fall into two categories:</p> <ul> <li> <p>An account assigned to a real person, which could be used by an end user to consume data or an administrator to manage the platform.</p> </li> <li> <p>An unattended account (a.k.a a service principal), which is not assigned to a real person, but used by another system or component of the platform itself which needs to perform a task.</p> </li> </ul>"},{"location":"infrastructure/identity/#user-accounts","title":"User Accounts","text":"<p>User accounts will be assigned for the following purposes:</p> <ul> <li>DeX Administrators</li> </ul> <p>Those who administer the platform. This is a highly privileged type of user and will be restricted to just a handful of individuals.</p> <ul> <li>DeX Developers</li> </ul> <p>Those who develop on the platform. This is also a privileged role, however restricted to creating, changing and configuring elements of the solution.</p> <ul> <li>DeX Consumers</li> </ul> <p>Those who query data from the platform.</p>"},{"location":"infrastructure/identity/#service-principals","title":"Service Principals","text":"<p>The following service principals have been created for the following purposes:</p> <ul> <li><code>sp-dex-deployment</code></li> </ul> <p>The account used to deploy the platform to Azure including the creation of services and the deployment/configuration of the solution.</p> <p>This is a highly privileged account which must be guarded.</p> <ul> <li><code>sp-dex-dev</code></li> </ul> <p>The account used by developers to access resources in Azure that are used during development, such as the development instance of Azure Container Registry.</p> <p>This account holds limited permissions and only has access to development related resources/data.</p> <ul> <li><code>sp-dex-provider-&lt;provider-code&gt;</code></li> </ul> <p>Each data provider will have their own account to push data into the platform, which is suffixed with their provider code (in the case of an NHS organisation, this will be their ODS code).</p> <p>The provider account will be limited to pushing data into the platform, and further to this will be restricted to specific domains of data, specific data types and specific message types.</p>"},{"location":"infrastructure/identity/#roles","title":"Roles","text":"<p>Roles grant a number of permissions to those which they are assigned and authorise them to perform a task.</p> <p>There are a number of in-built roles within Azure, however custom roles bespoke to the platform have been created which grant the specific permissions to perform a certain function.</p> <p>As well as permissions, a role can be associated with a number of claims, which the platform can use to further authorise an action. For example, a user may be in a role that provides the permission to push data to the platform, but the role must authorise a claim to push data with a specific organisation code.</p>"},{"location":"infrastructure/identity/#custom-roles","title":"Custom Roles","text":"<p>This section details the custom roles which have been created and their associated permissions/claims.</p>"},{"location":"infrastructure/identity/#dex-administrator","title":"Dex Administrator","text":"<p>Effectively the same as having the in-built \"Owner\" role over the Dex Azure subscription.</p>"},{"location":"infrastructure/identity/#dex-developer","title":"Dex Developer","text":"<p>Effectively the same as having the in-built \"Contributor\" role over the Dex Azure subscription.</p>"},{"location":"infrastructure/identity/#dex-provider","title":"Dex Provider","text":"<p>Each provider will have their Dex provider role, suffixed with their provider code (e.g. an ODS code for NHS organisations).</p> <p>The role will have the permissions to push data to the platform via the $ingest endpoint of the facade application.</p> <p>The role will have the following claims, which will be specific to the organisation and which data they are permitted to push:</p> <ul> <li>Organisation Code</li> <li>Source Domains (list)</li> <li>Input data types (list)</li> </ul>"},{"location":"infrastructure/identity/#dex-deployment","title":"Dex Deployment","text":"<p>Contains the permissions required to deploy the Dex solution into a dedicated resource group within the Dex Azure subscription.</p>"},{"location":"infrastructure/identity/#role-assignments","title":"Role Assignments","text":"<p>This section details the roles which have been assigned to certain accounts.</p> Account Role(s) (scope -&gt; service) <code>sp-dex-dev</code> - ACR Push (Resource -&gt; acrdexoss)- ACR Pull (Resource -&gt; acrdexoss) <code>sp-dex-deployment</code> - Key Vault Secrets Officer (Resource -&gt; kv-dex)- Storage Account Key Operator Service Role (Resource -&gt; sadextfstate)- FHIR Data Contributor (Subscription -&gt; Dorset Data Exchange) <code>sp-dex-provider-&lt;provider-code&gt;</code> - Dex Provider (Resource -&gt; app-dex-\\&lt;env&gt;)"},{"location":"infrastructure/overview/","title":"Overview","text":"<p>Our resources for DEX are described using Terraform, which is an infrastructure-as-code tool. This means that we can specify what the desired state of our system is, and Terraform will figure out how to create that state.</p> <p>This overview goes over some of the key concepts and files for understanding how the infrastructure is deployed. In particular, the following contents will provide an in-depth explanation of how the end-to-end process for our deployment works.</p> <p>We'll cover:</p> <ol> <li>How our API uses <code>tags</code> to add metadata to endpoints and routes.</li> <li>How our CI pipeline generates a Swagger file, which contains our Open-API specifications.</li> <li>How that same Swagger file is referenced to deploy our Azure API Management service.</li> <li>How the <code>tags</code> we defined are used to create policies in Azure API Management.</li> </ol>"},{"location":"infrastructure/overview/#contents","title":"Contents","text":"<ol> <li>The entry point - main.tf</li> <li>Where variables are declared - variables.tf</li> <li>Where variable values are provided - .tfvars</li> <li>A top-level resource - app-registration.tf</li> <li>Working with variables and values</li> <li>Dynamic creation of APIM policies using Terraform</li> <li>Putting it all together</li> </ol>"},{"location":"infrastructure/overview/#maintf","title":"main.tf","text":"<p>This is the primary entry point for our Terraform configuration. It is where we define the providers we use and Terraform modules we want to include in our deployment. For example, since we're creating our infrastructure on Azure, we use the <code>azurerm</code> provider. See docs Azure Provider.</p> <p>In addition, we also define different modules, these are self-contained packages of Terraform configurations that are managed as a group. This is done to improve modularity and maintainability by breaking configurations into smaller modular constructs.</p> <p>In our <code>main.tf</code> file, we reference a module for setting up a network, one for setting up our health services, and another for more general services we use. These are referenced from a subfolder for each. Note that, these modules can also have their own respective <code>main.tf</code> file, which differs to <code>main.tf</code> in the root.</p> <p>Each module consists of the variables that it needs. It is from the <code>main.tf</code> file that we specify a value for each of those variables. Consider this code snippet, and the different ways of assigning a variable a value:</p> <pre><code>resource \"azurerm_resource_group\" \"rg\" {\n  location = var.location\n  name     = \"rg-dex-${var.env}\"\n}\n\nmodule \"network\" {\n  source                              = \"./network\"\n  location                            = var.location\n  resource_group_name                 = azurerm_resource_group.rg.name\n}\n</code></pre> <ul> <li>The <code>source</code> argument is defined using a string.</li> <li>The <code>resource_group_name</code> variable is defined by referencing a resource we have already defined in the same module. That particular resource is the type \"azurerm_resource_group\" (see Terraform docs), which we have given the name \"rg\". That resource has <code>location</code> and <code>name</code> variables, so we can access the desired variable by referencing the resource type, resource name, and resource variable as <code>azurerm_resource_group.rg.name</code>.</li> </ul>"},{"location":"infrastructure/overview/#variablestf","title":"variables.tf","text":"<p>The variables declared in this file can be used across multiple Terraform files within the same module. They are typically used to customize aspects of the resources being created, such as their names, locations, sizes, and so on.</p> <p>Here's an example of what a variable declaration looks like in the <code>variables.tf</code> file:</p> <pre><code>variable \"location\" {\n  type = string\n}\n</code></pre> <p>In this example, a variable named <code>location</code> is declared and it's of type <code>string</code>.</p>"},{"location":"infrastructure/overview/#tfvars","title":"<code>.tfvars</code>","text":"<ul> <li>A <code>.tfvars</code> file (or Terraform Variables file) is used to set the values of the variables declared in our <code>variables.tf</code> file. The <code>.tfvars</code> file is a simple key-value store where the keys correspond to our variable names.</li> </ul> <p>For example in <code>tfvars.template</code>, we have the line:</p> <pre><code>location = \"UK South\"\n</code></pre> <p>For full instructions on how to use the <code>tfvars.template</code> see the <code>infrastructure/README.md</code>, which will give you instructions on how to use the <code>tfvars</code> for local development.</p> <p>A continuous integration (CI) and continuous deployment (CD) pipeline runs on each PR merge.</p> <p>More specifically, the CD pipeline is responsible for deployment, and the definition of the pipeline can be found in <code>.pipelines/cd-pipeline.yaml</code>. The stage \"DeployInfrastructure\" in the yaml file references the <code>.pipelines/templates/deploy-infrastructure.yaml</code> file.</p> <p>It is in this file where the values for the terraform variables are configured. Firstly, we login using the service principal, and add the build agent IP to keyvault so that the pipeline is able to access the necessary secrets.</p> <p>Next, a script is run which writes the values for each variable to a <code>tfvars</code> file for the given environment. This pulls values from the associated variables group in Azure DevOps. This means that values for variables can be kept in a single place and securely accessed.</p>"},{"location":"infrastructure/overview/#app-registrationtf","title":"app-registration.tf","text":"<p>Also in the top-level is the Terraform file for deploying the Application Registration which can be managed via Microsoft Entra ID. The documentation for the <code>azuread_application</code> resource can be found here.</p> <p>Some important things to note are:</p> <ul> <li>The application is deployed and we specify its identifier uri.</li> <li>The identifier uri must be globally unique, so we append a random string onto the URI.</li> <li>The random string is generated using a function at the top of the file.</li> </ul> <p>When creating it, ensure that a group of owners is assigned. This step is crucial because adding more owners (and consequently granting permissions) becomes challenging unless at least one owner is initially assigned. The <code>app_registration_owners</code> is set in the pipeline variables group, and should include the IDs of the users who should be assigned ownership.</p> <p>The resource includes the scope \"API.Call\" which can be granted to a user or application that has permissions to call the API.</p> <p>It also includes the App Roles that the application is created with, such as <code>DataProvider</code> and <code>DataConsumer</code>, which can be assigned to users or applications. This allows us to control which endpoints they can call, given their role.</p> <p>To add a new App Role, define which members can be assigned the role (User or Application), description and name, and a unique id, as shown below:</p> <pre><code>  app_role {\n    allowed_member_types = [\"User\", \"Application\"]\n    description          = \"DataAdministrator has full admin permissions\"\n    display_name         = \"DataAdministrator\"\n    enabled              = true\n    id                   = \"386395d3-901z-3156-ae07-34z0ba037z90\"\n    value                = \"DataAdministrator\"\n  }\n</code></pre> <p>Finally there is a pre-authorised application resource, the Azure CLI, which means that developers can use the Azure CLI to request an access token for the application registration.</p> <pre><code>resource \"azuread_application_pre_authorized\" \"azcli\" {\n  application_id       = azuread_application.app.id\n  authorized_client_id = var.azure_cli_client_id\n\n  permission_ids = flatten([\n    for api in azuread_application.app.api : [\n      for scope in api.oauth2_permission_scope : scope.id\n    ]\n  ])\n}\n</code></pre> <p>The app registration terraform file is in the top-level because we need to use the output in some of the follow-on modules. For example, in order to be able to create the policies in Azure API Management, we need to know the value of the application registration URI. We can only know the application registration URI after the application registration resource has been created, because of the requirement for a globally unique URI with a random guid.</p>"},{"location":"infrastructure/overview/#working-with-variables-and-values","title":"Working with variables and values","text":"<p>In Terraform, it's common to have multiple <code>variables.tf</code> files when you're working with modules. Each module in Terraform is a self-contained package of Terraform configurations that manages a collection of related resources, and each of these modules can have its own <code>variables.tf</code> file.</p> <p>The <code>variables.tf</code> file in each module is used to define the variables that are used in the module. This allows the module to accept inputs from the parent configuration or other modules.</p> <p>Here's an example of a directory structure with multiple modules, each having its own <code>variables.tf</code>:</p> <pre><code>main.tf\nvariables.tf\nnetwork/\n  - main.tf\n  - variables.tf\nservices/\n  - main.tf\n  - variables.tf\n  - api_management/\n    - main.tf\n    - variables.tf\n    - api_management_dex.tf\n</code></pre>"},{"location":"infrastructure/overview/#example-passing-the-app-reg-uri-between-modules","title":"Example: Passing the App Reg URI between modules","text":"<p>Using the example of the Application Registration resource, which is created at the top-most level, we'll see how the URI of that resource can be passed to other modules.</p> <p>First, if you open <code>main.tf</code>, you'll see that here the \"services\" module is declared, and values for the required variables are assigned. Here is a snippet, but the <code>app_registration_uri</code> is assigned the value of the app reg's <code>identifier_uri</code> (note: it's possible to assign multiple URIs, hence we take the first in the list):</p> <pre><code>module \"services\" {\n  app_registration_id              = azuread_application.app.client_id\n  app_registration_uri             = tolist(azuread_application.app.identifier_uris)[0]\n}\n</code></pre> <p>Next, the \"services\" module has a <code>main.tf</code> file, which in it includes the declaration for the Azure API Management module called \"api_management\". Here, in the same way, the app_registration_uri (which is now part of <code>variables.tf</code> within \"services\") can be accessed and assigned to the \"api_management\" module.</p> <pre><code>module \"api_management\" {\n  source                           = \"./api_management\"\n  app_registration_id              = var.app_registration_id\n  app_registration_uri             = var.app_registration_uri\n}\n</code></pre> <p>Then, within the \"api_management\" module, there is the file <code>api_management_dex.tf</code>, which defines the different APIM resources required for DEX. It is in this resource, which uses the <code>app_registration_uri</code> variable (which has been passed to it from the parent, \"services\"):</p> <pre><code>resource \"azurerm_api_management_named_value\" \"dex_app_id_uri\" {\n  name                = \"ApplicationIdUri\"\n  api_management_name = azurerm_api_management.apim.name\n  resource_group_name = var.resource_group_name\n  display_name        = \"ApplicationIdUri\"\n  value               = var.app_registration_uri\n}\n</code></pre> <p>Finally, that named value gets used in <code>policies/dex_operation_policy.tftpl</code>, e.g. we are able to inject the <code>ApplicationIdUri</code> like so:</p> <pre><code>&lt;audience&gt;{{ApplicationIdUri}}&lt;/audience&gt;\n</code></pre>"},{"location":"infrastructure/overview/#dynamic-creation-of-apim-policies-using-terraform","title":"Dynamic creation of APIM policies using Terraform","text":"<p>Azure API Management (APIM) policies are a powerful capability of the APIM service that allow the behaviour of the API operation to be modified without changing the code of the API itself. Policies are a series of statements that are executed sequentially on the request and/or response, and in DEX we use it for authentication and authorization - to check whether the user or application sending that request has permissions to access the endpoint. APIM policies are written in XML format.</p> <p>One downside of XML is that it is difficult to maintain and debug. This is relevant to our project as we have several endpoints, each with different policy requirements. Whilst some portions of the policy are similar, it would be difficult to maintain multiple XML files.</p> <p>Instead, we can use a Terraform Template file which has the extension <code>.tftpl</code>. A <code>.tftpl</code> file allows you to use interpolation syntax to embed variables or expressions that will be evaluated when the template is used in a Terraform configuration.</p> <p>In the \"api_management\" module, in the <code>locals.tf</code> file, you can see where the <code>dex_operation_policy</code> gets populated. What happens here is the <code>templatefile</code> function is used to render a template file with the provided variables (in this case, tags relating to the endpoint). This is done for each endpoint, so each endpoint has its own (custom) policy, which is determined by the tags.</p> <pre><code>locals {\n  apim_name      = \"apim-dex-${var.env}-${var.random_id}\"\n  apim_sku       = var.env == \"prd\" || var.env == \"stg\" ? \"Premium\" : \"Developer\"\n  apim_sku_name  = \"${local.apim_sku}_${var.apim_instance_count}\"\n  apim_dns_label = lower(replace(local.apim_name, \"-\", \"\"))\n  public_ip_name = \"apim-ip-dex-${var.env}-${var.random_id}\"\n\n  open_api_specification = jsondecode(file(\"${path.module}/dex-swagger.json\"))\n\n  operations = flatten([\n    for endpoint in local.open_api_specification.paths : [\n      for endpointType in endpoint : {\n        operationId = endpointType.operationId\n        policy = templatefile(\"${path.module}/policies/dex_operation_policy.tftpl\", {\n          tags = endpointType.tags\n        })\n      }\n    ]\n  ])\n}\n</code></pre>"},{"location":"infrastructure/overview/#how-are-tags-used-in-the-apim-policy","title":"How are tags used in the APIM policy","text":"<p>Let's see how the tags are used, and then return to where they come from.</p> <p>In the <code>dex_operation_policy.tftpl</code> file observe these two key areas.</p> <p>The first, we can see that if the provided \"tags\" variable contains the tag \"FhirBackend\", then it will inject a specific piece of XML.</p> <pre><code>        %{~ if contains(tags, \"FhirBackend\") ~}\n        &lt;set-backend-service backend-id=\"fhir-server\" /&gt;\n        %{~ endif ~}\n</code></pre> <p>Secondly, if a particular endpoint has a <code>RequiredRole=DexProvider</code> tag for example, then this snippet of code will read the tags, check if any of the tags contains the string \"RequiredRole\". If so, then for each of the tags, it will check if it is a \"RequiredRole\" tag, and then extract the role using the substring function:</p> <pre><code>            %{~ if strcontains((join(\";\", tags)), \"RequiredRole\")~}\n            &lt;required-claims&gt;\n                &lt;claim name=\"roles\" match=\"all\"&gt;\n                %{~ for tag in tags ~}\n                    %{~ if strcontains(tag, \"RequiredRole=\") ~}\n                    &lt;value&gt;${substr(tag, 13, length(tag) - 1)}&lt;/value&gt;\n                    %{~ endif ~}\n                %{~ endfor ~}\n                &lt;/claim&gt;\n            &lt;/required-claims&gt;\n            %{~ endif ~}\n</code></pre> <p>E.g. when applied to <code>RequiredRole=DexProvider</code>, the following code would extract <code>DexProvider</code>.</p> <pre><code>{substr(tag, 13, length(tag) - 1)}\n</code></pre> <p>After the template is used, the XML would be for example:</p> <pre><code>            &lt;required-claims&gt;\n                &lt;claim name=\"roles\" match=\"all\"&gt;\n                    &lt;value&gt;DexProvider&lt;/value&gt;\n                &lt;/claim&gt;\n            &lt;/required-claims&gt;\n</code></pre>"},{"location":"infrastructure/overview/#adding-tags-to-an-endpoint","title":"Adding tags to an endpoint","text":"<p>When we define an API endpoint, we can assign it tags. In <code>src/Api/Modules/IngestModule.cs</code> we define a route for handling HTTP POST requests to the <code>$ingest</code> endpoint.</p> <ul> <li>The first argument to the \"MapPost\" method is \"$ingest\", which represents the URL path for the route. In this case, the route will be triggered when a POST request is made to the \"/$ingest\" endpoint.</li> <li>The second argument is a method called \"Ingest\", which is the handler for the route. This method will be executed when the route is triggered.</li> <li>When you use the \"WithName\" method to assign a name to an operation, that name becomes the operation ID (here, \"PostIngest\"). The OperationId ensures that the right policies are applied to the right API operations during deployment.</li> <li>The \"WithTag\" method assigns tags, which in our case is where we specify the required role a user or application would need in order to make a POST request to this endpoint.</li> </ul> <pre><code>    public override void AddRoutes(IEndpointRouteBuilder app)\n    {\n        app.MapPost(\"$ingest\", Ingest).WithName(\"PostIngest\").WithTags(\"RequiredRole=DataProvider\");\n    }\n</code></pre> <p>Another example of this might be, if adding additional roles:</p> <pre><code>    public override void AddRoutes(IEndpointRouteBuilder app)\n    {\n        app.MapPost(\"$ingest\", Ingest).WithName(\"PostIngest\").WithTags(\"RequiredRole=DataProvider\",\"RequiredRole=DataAdministrator\");\n    }\n</code></pre> <p>As explained in swagger-file.md, as part of API Management we have a Swagger file which describes the API. The file is not stored in the repository and is instead generated when the CI pipeline is built.</p> <p>If you have access, you can view the latest swagger file:</p> <ol> <li>Go to Pipelines</li> <li>Click CI Pipeline</li> <li>Select the latest job run (or whichever you're interested in)</li> <li>Here you will find a \"Stages\" section, which contains a Services Build Verification stage, which should have 2 artifacts, click on \"2 artifacts\":</li> </ol> <p></p> <ol> <li>This will list the published artifacts and you should see SwaggerDoc. Clicking on this will download the swagger.json file that was generated as part of the CI build.</li> </ol> <p>An example is shown below, specifically, we show the part which relates to the <code>$ingest</code> endpoint:</p> <pre><code>\"/$ingest\": {\n      \"post\": {\n        \"tags\": [\n          \"RequiredRole=DataProvider\"\n        ],\n        \"operationId\": \"PostIngest\",\n</code></pre> <p>Our route has been created to support \"POST\" requests to the <code>$ingest</code> endpoint, and has the tags we added.</p>"},{"location":"infrastructure/overview/#putting-it-all-together","title":"Putting it all together","text":"<p>To summarise:</p> <ol> <li>In our API, we added the \"RequiredRole=DataProvider\" tag to a particular endpoint and route.</li> <li>When the CI pipeline is run, this generates a <code>swagger.json</code> file which contains our Open-API specifications.</li> <li>When the API Management service is created via terraform, it references the Open-API specifications from <code>swagger.json</code> to create the API routes.</li> <li>Also via terraform, the API Management policies are created, which are policies applied to each endpoint. These use the \"tags\" in the Swagger file to generate the correct policy definition, using the `dex_policy_operation.tftpl\" Terraform template.</li> <li>If we go to the Azure Portal, and look at our deployed Azure API Management service, by selecting \"APIs\" &gt; \"FHIR API\" &gt; \"PostIngest\" &gt; \"Policies\", we can see how the Terraform template has been populated with values for the tags from the Swagger file:</li> </ol> <p></p>"},{"location":"ingestion/csv-ingestion/","title":"CSV Ingestion","text":""},{"location":"ingestion/fhir-ingestion/","title":"FHIR Ingestion","text":""},{"location":"ingestion/hl7v2-ingestion/","title":"HL7v2 Ingestion","text":""},{"location":"ingestion/overview/","title":"Overview","text":""},{"location":"national-services/certificate-preparation/","title":"Certificate Preparation","text":"<p>The Healthcare Data Exchange uses signed JWT authentication to access some application-restricted RESTful APIs. Notably the Personal Demographics Service.</p> <p>NHS Digital provide extensive documentation explaining how this integration works.</p> <p>An important step in this process is generating and signing a JWT. This happens at runtime. The JWT is then used to authenticate with the PDS.</p> <p>In the <code>JwtHandler</code> we generate an instance of <code>SigningCredentials</code> using a <code>X509Certificate2</code> object. This certificate is stored in Azure Key Vault and loaded into the application container at runtime. Details of this process can be found at the ssl certificate configuration guide.</p>"},{"location":"national-services/certificate-preparation/#convert-certificate-to-pkcs12-x509-certificate","title":"Convert certificate to PKCS12 x509 certificate","text":"<p>Before importing the certificate into Azure Key Vault, it must be converted to a PKCS12 x509 certificate.</p> <p>The NHS Digital guide produces a self-signed certificate and a private key in X.509 PEM format. These components can be temporarily stored in <code>.pem</code> files and used to create a PKCS12 x509 certificate.</p> <p>The following commands can be used to convert the certificate and private key to a PKCS12 x509 certificate:</p> <pre><code>openssl x509 -in certificate.pem -out x509certificate.pem -signkey privatekey.pem\nopenssl pkcs12 -export -out certificate.pfx -in x509certificate.pem -inkey privatekey.pem\n</code></pre> <p>This command will prompt for a password to secure the PKCS12 x509 certificate. This password will be required when importing the certificate into Azure Key Vault.</p>"},{"location":"national-services/mesh/","title":"Mesh","text":"<p>The platform uses Mesh for a number of purposes such as extract of PDS bulk data and National Data Opt Out flags.</p> <p>This article describes how the platform uses Mesh and the steps required to setup and configure a new workflow.</p> <p>See this link for full details about Mesh.</p>"},{"location":"national-services/mesh/#mesh-api","title":"Mesh Api","text":"<p>The platform integrates with the Mesh Api which has a number of capabilities to send, track and receive messages</p> <p>See this link for full details about the Mesh Api.</p>"},{"location":"national-services/mesh/#nel-mesh-client","title":"NEL Mesh Client","text":"<p>The solution uses the open source North East London Mesh client - a dotnet sdk to facilitate connecting and interacting with Mesh.</p> <p>See this link to view the NEL Mesh client on github.</p>"},{"location":"national-services/mesh/#mailboxes-and-workflows","title":"Mailboxes and Workflows","text":"<p>All interactions with Mesh require a mailbox which consists of an inbox and an outbox. Messages are sent by uploading messages to the outbox, and retrieved by downloading messages from the inbox.</p> <p>Workflows define the type of data which will sent and received - such as a PDS bulk trace request, or an NDOP consent extract.</p> <p>See this link for details about the available workflows.</p> <p>It's best practice to have a single workflow per mailbox, therefore a new mailbox should be created when setting up a new workflow.</p> <p>See this link to access a form to apply for a new Mesh mailbox.</p>"},{"location":"national-services/mesh/#certificates","title":"Certificates","text":"<p>A certificate is required for each Mesh mailbox within an environment. They are supplied by NHS Digital, and requested by sending a certificate signing request (CSR) to the service desk.</p> <p>A mailbox must be created before requesting a certificate as the mailbox ID must be specified when generating a CSR.</p> <p>:warning: Be aware Mesh certificates expire after 3 years and must be renewed periodically.</p>"},{"location":"national-services/mesh/#how-to-request-a-mesh-certificate","title":"How to Request a Mesh Certificate","text":"<p>You will need to specify a keystore password that will be used throughout the process.</p>"},{"location":"national-services/mesh/#pre-requisites","title":"Pre-Requisites","text":"<ol> <li>Java Development Kit https://openjdk.org/</li> <li>OpenSSL https://www.openssl.org/</li> </ol>"},{"location":"national-services/mesh/#steps","title":"Steps","text":"<ol> <li>Generate a keystore for a mailbox:</li> </ol> <p><code>keytool -genkey -alias Meshclient -keyalg RSA -keysize 2048 -keystore C:\\temp\\&lt;mailbox-id&gt;.keystore -dname \"CN=&lt;mailbox-id&gt;.&lt;ods-code&gt;.api.Mesh-client.nhs.uk\"</code></p> <ol> <li>Create a CSR from the keystore</li> </ol> <p><code>keytool -certreq -alias Meshclient -keystore c:\\temp\\&lt;mailbox-id&gt;.keystore -file c:\\temp\\&lt;mailbox-id&gt;.csr</code></p> <ol> <li> <p>Send the CSR via email to NHS Digital service desk</p> </li> <li> <p>For the Mesh test environment email: <code>support.digitalservices@nhs.net</code></p> </li> <li>For the Mesh prod environment email: <code>ssd.nationalservicedesk@nhs.net</code></li> <li>In the body of the email, provide the following information:<ul> <li>Name</li> <li>Organisation</li> <li>Mailbox ID</li> <li>The certificate Common Name (CN): <code>&lt;mailbox-id&gt;.&lt;ods-code&gt;.api.Mesh-client.nhs.uk</code></li> <li>Reason for the Certificate (such as, New Mesh client, Current certificate has or is about to expire)</li> </ul> </li> <li> <p>When a response has been received, save the certificate into <code>c:\\temp\\&lt;mailbox-id.crt</code></p> </li> <li> <p>Export private key from keystore and convert it to an unencrypted PEM key</p> </li> </ol> <p><code>keytool -importkeystore -srckeystore C:\\temp\\&lt;mailbox-id&gt;.keystore -destkeystore C:\\temp\\&lt;mailbox-id&gt;.p12 -deststoretype PKCS12 -srcalias Meshclient</code></p> <p><code>openssl pkcs12 -in C:\\temp\\&lt;mailbox-id&gt;.p12\" -nodes -nocerts -out C:\\temp\\&lt;mailbox-id&gt;-private-key.pem</code></p> <ol> <li>Create a PFX file containing the certificate and the private key</li> </ol> <p><code>openssl pkcs12 -in c:\\temp\\&lt;mailbox-id&gt;.crt -inkey c:\\temp\\&lt;mailbox-id&gt;-private-key.pem -export -out c:\\temp\\&lt;mailbox-id&gt;.pfx</code></p> <ol> <li>In some cases you may to convert the PFX to a base 64 string (e.g. to use within as a configuration value in appsettings), in which case PowerShell can be used</li> </ol> <p><code>$fileContentBytes = get-content C:\\temp\\&lt;mailbox-id&gt;.pfx -Encoding Byte[System.Convert]::ToBase64String($fileContentBytes) | Out-File c:\\temp\\&lt;mailbox-id&gt;-pfx.txt</code></p> <ol> <li>Store the PFX file and keystore password in Azure Key Vault</li> </ol>"},{"location":"national-services/national-data-opt-out/","title":"National Data Opt-Out","text":"<p>The data exchange will integrate with the NDOP APIs to import Consent information and keep the information up to date.</p>"},{"location":"national-services/national-data-opt-out/#apis","title":"APIs","text":"<p>See the following links for details about the APIs:</p> <ul> <li>NDOP FHIR API - https://digital.nhs.uk/developer/api-catalogue/national-data-opt-out-fhir</li> <li>NDOP MESH API - https://digital.nhs.uk/developer/api-catalogue/national-data-opt-out-service-mesh</li> </ul> <p>NOTE: Currently there are no plans to integrate with the NDOP FHIR API.</p>"},{"location":"national-services/national-data-opt-out/#ndop-mesh","title":"NDOP MESH","text":"<p>The NDOP MESH API is used for refreshing the Consent resources persisted in the system as a batch process, executed on a regular schedule.</p> <p>The interactions with the API involve uploading a request to the outbox of the MESH mailbox linked to the NDOP MESH workflow, and polling the inbox to check for a response. Once a response is available, it is downloaded and processed by the application.</p>"},{"location":"national-services/national-data-opt-out/#request","title":"Request","text":"<p>The request consists of a data file which contains the patients that we require updated consent information for and a corresponding control file.</p> <p>The data file must contain the fields specified by the NDOP MESH workflow, and the control file must contain metadata specific to the request.</p>"},{"location":"national-services/national-data-opt-out/#data-file","title":"Data File","text":"<p>The data file consists of two fields - the NHS Number and an empty field. An example of a data line is: <code>1234567890,\\r\\n</code></p> <p>Details of the format of the data file can be seen here:</p> <p>https://digital.nhs.uk/services/national-data-opt-out/compliance-with-the-national-data-opt-out/check-for-national-data-opt-outs-service#create-the-dat-file-of-patient-data-to-send</p>"},{"location":"national-services/national-data-opt-out/#control-file","title":"Control File","text":"<p>Details of the format of the control file can be seen here:</p> <p>https://digital.nhs.uk/services/message-exchange-for-social-care-and-health-mesh/mesh-guidance-hub/client-user-guide</p> <p>The specific values which must go into control file for NDOP can be seen here:</p> <p>https://digital.nhs.uk/services/national-data-opt-out/compliance-with-the-national-data-opt-out/check-for-national-data-opt-outs-service#send-and-receive-your-file-over-mesh-client-api</p>"},{"location":"national-services/national-data-opt-out/#response","title":"Response","text":"<p>The response consists of a single CSV file containing the patients which have not opted out. The response must be processed alongside the request, in order to know who has opted out and update the FHIR store accordingly.</p>"},{"location":"national-services/organisation-data-service/","title":"Organisation Data Service","text":""},{"location":"national-services/personal-demographics-service/","title":"Personal Demographics Service","text":"<p>The data exchange will integrate with the PDS APIs to import patients and keep their information up to date.</p>"},{"location":"national-services/personal-demographics-service/#design","title":"Design","text":"<p>This diagram shows the intended architecture for integration with the PDS APIs, including the persistence of PDS data and how the persisted data will be refreshed.</p> <p></p>"},{"location":"national-services/personal-demographics-service/#apis","title":"APIs","text":"<p>See the following links for details about the APIs:</p> <ul> <li>PDS FHIR API - https://digital.nhs.uk/developer/api-catalogue/personal-demographics-service-fhir</li> <li>PDS MESH API - https://digital.nhs.uk/developer/api-catalogue/personal-demographic-service-mesh</li> <li>PDS NEMS API - https://digital.nhs.uk/developer/api-catalogue/personal-demographics-service-notifications-fhir</li> </ul>"},{"location":"national-services/personal-demographics-service/#test-data","title":"Test Data","text":"<p>PDS test data is generated in the NHSD Test Data Self Service Portal. The portal will generate a number of test patients and import them into the specified PDS test environments. The patients can then be downloaded from the portal in Excel format.</p> <p>The test data generated for this project can be found in the test data folder.</p>"},{"location":"national-services/personal-demographics-service/#test-data-self-service-portal","title":"Test Data Self Service Portal","text":"<p>The portal can be accessed at the following URL:</p> <p>http://testdatacloud.co.uk:8080/ords/apex-prod/f?p=159:LOGIN</p> <p>The username/password are stored in the development key vault.</p>"},{"location":"national-services/personal-demographics-service/#pds-mesh","title":"PDS MESH","text":"<p>The PDS MESH API is used for refreshing the Patient resources persisted in the system as a batch process, executed on a regular schedule.</p> <p>The interactions with the API involve uploading a request to the outbox of the MESH mailbox linked to the PDS MESH workflow, and polling the inbox to check for a response. Once a response is available, it is downloaded and processed by the application.</p>"},{"location":"national-services/personal-demographics-service/#request","title":"Request","text":"<p>The request consists of a single CSV which contains the patients that we require updated details for. It must contain the fields specified by the PDS MESH workflow. Details of the format of the request file can be seen here:</p> <p>https://digital.nhs.uk/developer/api-catalogue/personal-demographic-service-mesh/pds-mesh-data-dictionary#request-file-format</p>"},{"location":"national-services/personal-demographics-service/#response","title":"Response","text":"<p>The response consists of a single CSV file containing the latest information for each patient included in the request file. Details of the format of the response file can be seen here:</p> <p>https://digital.nhs.uk/developer/api-catalogue/personal-demographic-service-mesh/pds-mesh-data-dictionary#response-file</p> <p>In addition, the response file will include a code to indicate whether a record match was found. A table with the response codes (including at a person level) can be found here:</p> <p>https://digital.nhs.uk/developer/api-catalogue/personal-demographic-service-mesh/pds-mesh-data-dictionary#response-codes</p>"},{"location":"national-services/personal-demographics-service/#handling-invalid-records","title":"Handling invalid records","text":"<p>The PDS MESH person level response codes include an invalid flag.</p> <p>The invalid flag may be used when an NHS number is being deleted/discontinued. This could be for legal or safeguarding reasons, for example adoption, gender reassignment, other protected person such as witness protection or where there are possible data quality issues that have been dealt with.</p> <p>In the case of duplicates, records are merged, one number is retained, and the other is superseded.  PDS retains a systemic link between the two and if someone traces a patient using the superseded number, the PDS record for the retained number will be returned.</p> <p>Retrieving and accessing a record with an invalid flag means:</p> <ul> <li>a discontinued NHS number has been used or a record is no longer in use, (for example where two patient records have been merged and one is no longer used) in which case the NHS number that should be used will be returned</li> <li>the healthcare professional will not be able to see any demographic information for the patient and a message will be displayed indicating the record is not valid and that they should search for the correct demographic record, the searcher gets a \u2018No trace\u2019 response</li> </ul> <p>Source: https://www.england.nhs.uk/long-read/personal-demographic-service-pds/</p> <p>In DEX, we handle merging and deleting records to reflect the demographics from PDS. This is done based on the following:</p> <ul> <li>When tracing is successful and a match is found, but the record has a confidentiality status of \u2018I\u2019 (invalid), certain patient details are suppressed.</li> <li>Specifically, all information related to the patient, including the NHS number, is hidden.</li> <li>The resulting response for such invalid records appears as follows:</li> </ul> <p><code>02403456-031f-11e7-a926-080027a2de00,9991112758,,,,,,,,,,,,,,,,91,0000000000,1,0,0,0,0,0,</code></p> <ul> <li>The field that needs to be checked for matching indicators is the one containing the NHS number.</li> <li>If there is a match with any of the following values, the record has not been successfully matched:</li> <li><code>0000000000</code>: No match was found.</li> <li><code>9999999999</code>: Multiple matches were found.</li> <li><code>&lt;blank&gt;</code>: Not enough fields provided for the trace.</li> </ul> <p>Other fields are left empty or contain default values.</p> <p>Source: https://digital.nhs.uk/developer/api-catalogue/personal-demographic-service-mesh/pds-mesh-data-dictionary#response-codes</p>"}]}